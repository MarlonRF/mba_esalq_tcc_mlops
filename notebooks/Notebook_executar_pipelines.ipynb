{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e511e6c",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bab299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar dados + \n",
    "import sys\n",
    "sys.path.append('..')  # Adiciona o diret√≥rio pai\n",
    "from src.utils.io.io_local import *\n",
    "from src.utils.io.io_clearml import *\n",
    "\n",
    "from src.utils.io import load_dataframe\n",
    "from config import config_custom as config\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_features import executar_pipeline_features\n",
    "from src.pipelines import treinar_pipeline_completo, treinar_rapido\n",
    "\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d914c",
   "metadata": {},
   "source": [
    "# 1. Pipelines (local/standalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bad628",
   "metadata": {},
   "source": [
    "## 1.1 Pipeline processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5562b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar dados + # 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')\n",
    "\n",
    "# 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_proc = executar_pipeline_processamento(\n",
    "    df_raw,\n",
    "    config_imputacao_customizada=config.CONFIG_IMPUTACAO_CUSTOMIZADA,\n",
    "    criar_agrupamento_temporal=True,\n",
    "    nome_coluna_agrupamento='mes-ano'\n",
    ")\n",
    "print(f'Ap√≥s processamento: {df_proc.shape}')\n",
    "print(f'NAs restantes: {df_proc.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca17e6",
   "metadata": {},
   "source": [
    "## 1.2 Pipeline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5eaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURES (Codifica√ß√£o + Derivadas + Normaliza√ß√£o)\n",
    "df_feat, artefatos = executar_pipeline_features(\n",
    "    df_proc,\n",
    "    # Codifica√ß√£o\n",
    "    aplicar_codificacao=True,\n",
    "    metodo_codificacao='label',           # 'label' ou 'onehot'\n",
    "    sufixo_codificacao='_cod',\n",
    "    \n",
    "    # Features derivadas\n",
    "    criar_features_derivadas=True,\n",
    "    tipos_features_derivadas=[\n",
    "        'imc',                            # √çndice de Massa Corporal\n",
    "        'imc_classe',                     # Classe do IMC\n",
    "        'heat_index',                     # √çndice de calor\n",
    "        'dew_point',                      # Ponto de orvalho\n",
    "        't*u',                            # Temperatura √ó Umidade\n",
    "    ],\n",
    "    \n",
    "    # Normaliza√ß√£o\n",
    "    aplicar_normalizacao=True,\n",
    "    metodo_normalizacao='standard',       # 'standard', 'minmax', 'robust'\n",
    "    agrupamento_normalizacao='mes-ano',   # Normalizar por grupo\n",
    "    sufixo_normalizacao='_norm',\n",
    ")\n",
    "\n",
    "print(f'Ap√≥s features: {df_feat.shape}')\n",
    "print(f'Artefatos criados: {list(artefatos.keys())}')\n",
    "df_feat.to_csv(\"../dados/resultados/dados_processados_novas_features.csv\")\n",
    "# Visualizar resultado\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55638322",
   "metadata": {},
   "source": [
    "## 1.3 Pipeline de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_gerais import PARAMS_PADRAO\n",
    "\n",
    "# Definir coluna alvo e features\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "# Usar apenas features principais para teste\n",
    "features_treino = [\n",
    "    'idade', 'sexo_cod', 'peso', 'altura',\n",
    "    'tmedia', 'ur', 'vel_vento',\n",
    "]\n",
    "\n",
    "tipos_modelos ='regressao'\n",
    "\n",
    "# Filtrar features que existem no DataFrame\n",
    "features_existentes = [f for f in features_treino if f in df_feat.columns]\n",
    "print(f\"Features para treinamento: {features_existentes}\")\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_feat[features_existentes + [coluna_alvo]].dropna()\n",
    "print(f\"Dataset de treino: {df_treino.shape}\")\n",
    "\n",
    "# 4. TREINAMENTO\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ INICIANDO PIPELINE DE TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema=tipos_modelos,  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTADOS DO TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nome do melhor modelo\n",
    "nome_modelo = str(resultado['tabela_comparacao'].index[0])\n",
    "print(f\"\\n‚úì Melhor modelo: {nome_modelo}\")\n",
    "\n",
    "print(f\"\\nüìà M√©tricas principais:\")\n",
    "metricas = resultado['metricas_melhor']\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        print(f\"  ‚Ä¢ {nome}: {valor:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Modelo salvo em: {resultado.get('caminho_modelo', 'N/A')}\")\n",
    "\n",
    "print(\"\\nüìã Compara√ß√£o de modelos (top 5 m√©tricas):\")\n",
    "print(resultado['tabela_comparacao'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_10_experimentos = {}\n",
    "for i in range(10):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ü§ñ INICIANDO EXPERIMENTO {i+1}/10\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "    \n",
    "    resultados_10_experimentos[f'Experimento_{i+1}'] = resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_comparacao = [dicionario[\"tabela_comparacao\"] for dicionario in resultados_10_experimentos.values()] \n",
    "tabelas_comparacao =  [tabela.rename(columns={'Model':'Modelos','Accuracy': 'Acur√°cia', 'AUC': 'AUC', 'Recall': 'Recall', 'Prec.': 'Prec.', 'F1': 'F1'}) for tabela in tabelas_comparacao]\n",
    "serie_nomes_modelos = tabelas_comparacao[0]['Modelos']\n",
    "tabelas_comparacao = [tabela.select_dtypes(include='number') for tabela in tabelas_comparacao] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_desvio_metricas= pd.concat(tabelas_comparacao).groupby(level=0).std()\n",
    "df_media_metricas = pd.concat(tabelas_comparacao).groupby(level=0).mean()\n",
    "df_media_desvio_metricas_str = df_media_metricas.round(2).astype(str) + \" ¬± \" + df_desvio_metricas.round(2).astype(str)\n",
    "df_media_desvio_metricas_str = df_media_desvio_metricas_str.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_media_metricas = df_media_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas = df_desvio_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas.to_csv('../dados/resultados/desvio_metricas.csv')\n",
    "df_media_metricas.to_csv('../dados/resultados/media_metricas.csv')\n",
    "df_media_desvio_metricas_str.to_csv('../dados/resultados/media_e_desvio_metricas_str.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_media_desvio_metricas_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c814466",
   "metadata": {},
   "source": [
    "# Pipelines com integra√ß√£o ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PIPELINE COMPLETO COM CLEARML - REGISTRO TOTAL\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, OutputModel\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports dos pipelines\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_treinamento_unified import treinar_pipeline_completo\n",
    "\n",
    "# Configura√ß√£o\n",
    "PROJECT_NAME = \"conforto_termico\"\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLETO COM CLEARML - FULL TRACKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 1: UPLOAD DE DADOS BRUTOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 1] Upload de Dados Brutos para ClearML\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar dataset de dados brutos\n",
    "dataset_bruto = Dataset.create(\n",
    "    dataset_name=\"dados_brutos_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    description=\"Dados brutos de conforto termico de Santa Maria\"\n",
    ")\n",
    "\n",
    "# Salvar temporariamente para upload\n",
    "temp_path = Path(\"../dados/temp_clearml\")\n",
    "temp_path.mkdir(exist_ok=True)\n",
    "arquivo_bruto = temp_path / \"dados_brutos.csv\"\n",
    "df_raw.to_csv(arquivo_bruto, index=False)\n",
    "\n",
    "# Adicionar arquivo ao dataset\n",
    "dataset_bruto.add_files(str(arquivo_bruto))\n",
    "dataset_bruto.upload()\n",
    "dataset_bruto.finalize()\n",
    "\n",
    "print(f\"Dataset bruto criado: ID = {dataset_bruto.id}\")\n",
    "print(f\"  - Shape: {df_raw.shape}\")\n",
    "print(f\"  - Colunas: {len(df_raw.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 2: PIPELINE DE PROCESSAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 2] Pipeline de Processamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de processamento (COM FIX PARA NOTEBOOKS)\n",
    "task_processamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Processamento\",\n",
    "    task_type=Task.TaskTypes.data_processing,\n",
    "    reuse_last_task_id=False,  # N√£o reusar task anterior\n",
    "    continue_last_task=False,   # Criar nova task sempre\n",
    "    auto_connect_frameworks=False,  # Desabilita captura autom√°tica\n",
    "    auto_resource_monitoring=False  # Desabilita monitoramento de recursos\n",
    ")\n",
    "\n",
    "# Conectar dataset de entrada\n",
    "task_processamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_bruto.id,\n",
    "    \"dataset_name\": \"dados_brutos_conforto_termico\"\n",
    "})\n",
    "\n",
    "# Executar processamento\n",
    "print(\"Executando processamento...\")\n",
    "df_processado = executar_pipeline_processamento(df_raw)\n",
    "\n",
    "# Registrar m√©tricas de processamento\n",
    "task_processamento.get_logger().report_single_value(\"linhas_entrada\", df_raw.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"linhas_saida\", df_processado.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_entrada\", df_raw.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_saida\", df_processado.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"nas_removidos\", df_raw.isna().sum().sum() - df_processado.isna().sum().sum())\n",
    "\n",
    "print(f\"Dados processados: {df_processado.shape}\")\n",
    "print(f\"NAs restantes: {df_processado.isna().sum().sum()}\")\n",
    "\n",
    "# Upload de dados processados como dataset\n",
    "dataset_processado = Dataset.create(\n",
    "    dataset_name=\"dados_processados_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    parent_datasets=[dataset_bruto.id],\n",
    "    description=\"Dados processados (sem NAs, limpeza aplicada)\"\n",
    ")\n",
    "\n",
    "arquivo_processado = temp_path / \"dados_processados.csv\"\n",
    "df_processado.to_csv(arquivo_processado, index=False)\n",
    "dataset_processado.add_files(str(arquivo_processado))\n",
    "dataset_processado.upload()\n",
    "dataset_processado.finalize()\n",
    "\n",
    "print(f\"Dataset processado criado: ID = {dataset_processado.id}\")\n",
    "\n",
    "# Registrar como artefato na task\n",
    "task_processamento.upload_artifact(\n",
    "    \"dados_processados_sample\",\n",
    "    artifact_object=df_processado.head(100)  # Apenas amostra\n",
    ")\n",
    "\n",
    "task_processamento.close()\n",
    "print(\"Task de processamento finalizada!\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 3: PIPELINE DE TREINAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 3] Pipeline de Treinamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de treinamento (COM FIX PARA NOTEBOOKS)\n",
    "task_treinamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Treinamento\",\n",
    "    task_type=Task.TaskTypes.training,\n",
    "    reuse_last_task_id=False,\n",
    "    continue_last_task=False,\n",
    "    auto_connect_frameworks=False,\n",
    "    auto_resource_monitoring=False\n",
    ")\n",
    "\n",
    "# Conectar dataset processado\n",
    "task_treinamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_processado.id,\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"tipo_problema\": \"regressao\"\n",
    "})\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_processado.dropna()\n",
    "print(f\"Dados para treino: {df_treino.shape}\")\n",
    "\n",
    "# Executar treinamento\n",
    "print(\"Executando treinamento...\")\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',\n",
    "    n_modelos_comparar=3,\n",
    "    otimizar_hiperparametros=True,\n",
    "    n_iter_otimizacao=10,\n",
    "    salvar_modelo_final=True,\n",
    "    nome_modelo=\"modelo_conforto_termico_clearml\",\n",
    "    pasta_modelos=\"../modelos\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 4: REGISTRO DE M√âTRICAS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 4] Registro de M√©tricas\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "logger = task_treinamento.get_logger()\n",
    "\n",
    "# M√©tricas do melhor modelo\n",
    "metricas = resultado['metricas_melhor']\n",
    "print(\"\\nMetricas registradas:\")\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        logger.report_single_value(nome, valor)\n",
    "        print(f\"  {nome}: {valor:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 5: UPLOAD DE TABELAS E ARTEFATOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 5] Upload de Tabelas e Artefatos\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tabela de compara√ß√£o de modelos\n",
    "tabela_comparacao = resultado['tabela_comparacao']\n",
    "task_treinamento.upload_artifact(\n",
    "    \"comparacao_modelos\",\n",
    "    artifact_object=tabela_comparacao\n",
    ")\n",
    "print(\"Tabela de comparacao enviada como artefato\")\n",
    "\n",
    "# Criar gr√°fico de compara√ß√£o de modelos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "tabela_comparacao[['MAE', 'MSE', 'RMSE', 'R2']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Comparacao de Modelos - Metricas')\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Valor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Enviar gr√°fico\n",
    "logger.report_matplotlib_figure(\n",
    "    title=\"Comparacao de Modelos\",\n",
    "    series=\"Metricas\",\n",
    "    figure=fig,\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "print(\"Grafico de comparacao enviado\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 6: REGISTRO DO MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 6] Registro do Modelo\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar OutputModel\n",
    "melhor_modelo_nome = str(resultado['tabela_comparacao'].index[0])\n",
    "output_model = OutputModel(\n",
    "    task=task_treinamento,\n",
    "    name=f\"modelo_{melhor_modelo_nome}_conforto_termico\",\n",
    "    framework=\"PyCaret\"\n",
    ")\n",
    "\n",
    "# Registrar informa√ß√µes do modelo\n",
    "output_model.update_labels({\n",
    "    \"tipo\": \"regressao\",\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"melhor_modelo\": melhor_modelo_nome\n",
    "})\n",
    "\n",
    "# Adicionar m√©tricas ao modelo\n",
    "output_model.update_design(config_dict={\n",
    "    \"metricas\": {k: float(v) for k, v in metricas.items() if isinstance(v, (int, float))},\n",
    "    \"n_features\": df_treino.shape[1] - 1,\n",
    "    \"n_samples_treino\": df_treino.shape[0]\n",
    "})\n",
    "\n",
    "# Upload do arquivo do modelo\n",
    "if 'caminho_modelo' in resultado and resultado['caminho_modelo']:\n",
    "    caminho_modelo = Path(resultado['caminho_modelo'])\n",
    "    if caminho_modelo.exists():\n",
    "        output_model.update_weights(weights_filename=str(caminho_modelo))\n",
    "        print(f\"Modelo registrado: {caminho_modelo}\")\n",
    "    else:\n",
    "        print(f\"AVISO: Arquivo do modelo nao encontrado: {caminho_modelo}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 7: FINALIZA√á√ÉO E RESUMO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 7] Finalizacao\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tags na task\n",
    "task_treinamento.add_tags([\n",
    "    \"producao\",\n",
    "    melhor_modelo_nome,\n",
    "    f\"r2_{metricas.get('R2', 0):.2f}\".replace(\".\", \"_\")\n",
    "])\n",
    "\n",
    "task_treinamento.close()\n",
    "\n",
    "# Limpar arquivos tempor√°rios\n",
    "import shutil\n",
    "if temp_path.exists():\n",
    "    shutil.rmtree(temp_path)\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMO FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETO FINALIZADO - TUDO REGISTRADO NO CLEARML!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProjeto: {PROJECT_NAME}\")\n",
    "print(f\"\\nDatasets criados:\")\n",
    "print(f\"  1. Dados brutos: {dataset_bruto.id}\")\n",
    "print(f\"  2. Dados processados: {dataset_processado.id}\")\n",
    "print(f\"\\nTasks criadas:\")\n",
    "print(f\"  1. Processamento: {task_processamento.id}\")\n",
    "print(f\"  2. Treinamento: {task_treinamento.id}\")\n",
    "print(f\"\\nModelo registrado:\")\n",
    "print(f\"  - Nome: modelo_{melhor_modelo_nome}_conforto_termico\")\n",
    "print(f\"  - Melhor modelo: {melhor_modelo_nome}\")\n",
    "print(f\"  - R2: {metricas.get('R2', 0):.4f}\")\n",
    "print(f\"  - MAE: {metricas.get('MAE', 0):.4f}\")\n",
    "print(\"\\nAcesse o ClearML UI para visualizar todos os resultados!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba750975",
   "metadata": {},
   "source": [
    "## [OPCIONAL] Buscar e Reusar Recursos do ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# REUSAR RECURSOS DO CLEARML (Datasets, Modelos, Tasks)\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, Model\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_NAME = \"conforto_termico_def\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUSCANDO RECURSOS NO CLEARML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LISTAR DATASETS DISPON√çVEIS\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Datasets disponiveis no projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "datasets = Dataset.list_datasets(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    partial_name=\"conforto_termico\"\n",
    ")\n",
    "\n",
    "for i, ds in enumerate(datasets, 1):\n",
    "    print(f\"\\n{i}. {ds.name}\")\n",
    "    print(f\"   ID: {ds.id}\")\n",
    "    print(f\"   Versao: {ds.version}\")\n",
    "    print(f\"   Criado: {ds.created}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. BAIXAR DATASET ESPEC√çFICO\n",
    "# ============================================================================\n",
    "print(\"\\n[2] Baixando dataset processado:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Buscar o dataset mais recente de dados processados\n",
    "dataset_processado = Dataset.get(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    dataset_name=\"dados_processados_conforto_termico\"\n",
    ")\n",
    "\n",
    "# Baixar para pasta local\n",
    "local_path = dataset_processado.get_local_copy()\n",
    "print(f\"Dataset baixado para: {local_path}\")\n",
    "\n",
    "# Carregar dados\n",
    "arquivos = list(Path(local_path).glob(\"*.csv\"))\n",
    "if arquivos:\n",
    "    df_downloaded = pd.read_csv(arquivos[0])\n",
    "    print(f\"Dados carregados: {df_downloaded.shape}\")\n",
    "    print(df_downloaded.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LISTAR TASKS DO PROJETO\n",
    "# ============================================================================\n",
    "print(\"\\n[3] Tasks do projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "tasks = Task.get_tasks(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=None  # Todas as tasks\n",
    ")\n",
    "\n",
    "for i, t in enumerate(tasks[:5], 1):  # Primeiras 5\n",
    "    print(f\"\\n{i}. {t.name}\")\n",
    "    print(f\"   ID: {t.id}\")\n",
    "    print(f\"   Status: {t.status}\")\n",
    "    print(f\"   Tipo: {t.task_type}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUSCAR MODELOS REGISTRADOS\n",
    "# ============================================================================\n",
    "print(\"\\n[4] Modelos registrados:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "modelos = Model.query_models(\n",
    "    project_name=PROJECT_NAME,\n",
    "    model_name=None  # Todos os modelos\n",
    ")\n",
    "\n",
    "for i, modelo in enumerate(modelos, 1):\n",
    "    print(f\"\\n{i}. {modelo.name}\")\n",
    "    print(f\"   ID: {modelo.id}\")\n",
    "    print(f\"   Framework: {modelo.framework}\")\n",
    "    print(f\"   Labels: {modelo.labels}\")\n",
    "    \n",
    "    # Baixar modelo se quiser\n",
    "    # modelo_local = modelo.get_local_copy()\n",
    "    # print(f\"   Download: {modelo_local}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CLONAR E REUSAR TASK\n",
    "# ============================================================================\n",
    "print(\"\\n[5] Exemplo: Clonar uma task existente\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if tasks:\n",
    "    task_original = tasks[0]\n",
    "    print(f\"Clonando task: {task_original.name}\")\n",
    "    \n",
    "    # Clonar task (cria uma c√≥pia)\n",
    "    task_clonada = Task.clone(\n",
    "        source_task=task_original.id,\n",
    "        name=f\"{task_original.name}_clonada\",\n",
    "        project=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Task clonada criada: {task_clonada.id}\")\n",
    "    print(\"Voc√™ pode executar esta task clonada com configuracoes diferentes!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSCA DE RECURSOS CONCLUIDA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cffb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] Testando carregamento de credenciais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:31:24,996 - INFO - M√≥dulo ClearML carregado com sucesso\n",
      "2026-02-10 12:31:24,999 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-10 12:31:25,000 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-10 12:31:25,001 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-10 12:31:25,001 - INFO - ‚úì ClearML configurado para uso ONLINE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Credenciais carregadas com sucesso\n",
      "\n",
      "[3] Testando importa√ß√£o do pipeline...\n",
      "‚úì Pipeline importado com sucesso\n",
      "\n",
      "[4] Verificando arquivo de dados...\n",
      "‚úì Arquivo encontrado: C:\\Arquivos_Pessoais\\PROJETOS\\tcc_mba_esalq_mlops\\dados\\2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "  Tamanho: 0.29 MB\n",
      "\n",
      "================================================================================\n",
      "Se tudo estiver ‚úì, execute a pr√≥xima c√©lula para rodar o pipeline\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# PASSO 1: Verificar Importa√ß√µes e Credenciais\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar path do projeto\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Testar carregamento de credenciais\n",
    "print(\"\\n[2] Testando carregamento de credenciais...\")\n",
    "try:\n",
    "    from src.clearml.utils.credenciais_clearml import configurar_clearml_online\n",
    "    sucesso = configurar_clearml_online()\n",
    "    if sucesso:\n",
    "        print(\"‚úì Credenciais carregadas com sucesso\")\n",
    "    else:\n",
    "        print(\"‚úó Falha ao carregar credenciais\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao importar/configurar: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 3. Verificar importa√ß√£o do pipeline\n",
    "print(\"\\n[3] Testando importa√ß√£o do pipeline...\")\n",
    "try:\n",
    "    from src.clearml.pipelines_clearml import executar_pipeline_processamento_clearml\n",
    "    print(\"‚úì Pipeline importado com sucesso\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao importar pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. Verificar arquivo de dados\n",
    "print(\"\\n[4] Verificando arquivo de dados...\")\n",
    "arquivo_dados = Path('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "if arquivo_dados.exists():\n",
    "    print(f\"‚úì Arquivo encontrado: {arquivo_dados.resolve()}\")\n",
    "    print(f\"  Tamanho: {arquivo_dados.stat().st_size / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚úó Arquivo N√ÉO encontrado: {arquivo_dados.resolve()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Se tudo estiver ‚úì, execute a pr√≥xima c√©lula para rodar o pipeline\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14948102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=33e72bc5f5b14b8fae2b2335b2e97e15\n",
      "ClearML results page: https://app.clear.ml/projects/ff08d1c0eb5e473885441836f34c7e39/experiments/33e72bc5f5b14b8fae2b2335b2e97e15/output/log\n",
      "2026-02-10 12:31:57,689 - clearml.Task - INFO - Waiting for repository detection and full package requirement analysis\n",
      "2026-02-10 12:32:55,157 - clearml.Task - INFO - Finished repository detection and package analysis\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from clearml import Task\n",
    "\n",
    "# O ClearML vai usar as vari√°veis de ambiente automaticamente\n",
    "task = Task.init(project_name='teste', task_name='meu_teste')\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2b9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:33:04,058 - INFO - Modo ONLINE: Iniciando integra√ß√£o ClearML\n",
      "2026-02-10 12:33:04,065 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-10 12:33:04,065 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-10 12:33:04,065 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-10 12:33:04,065 - INFO - ‚úì ClearML configurado para uso ONLINE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=877bf2def79d4e66b6b53d22beb00aea\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/877bf2def79d4e66b6b53d22beb00aea/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:33:35,556 - INFO - Task criada: Pipeline_Processamento (ID: 877bf2def79d4e66b6b53d22beb00aea, Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS)\n",
      "2026-02-10 12:34:25,843 - INFO - ‚úì Task ClearML criada (ID: 877bf2def79d4e66b6b53d22beb00aea)\n",
      "2026-02-10 12:34:25,844 - INFO - ================================================================================\n",
      "2026-02-10 12:34:25,845 - INFO - PIPELINE DE PROCESSAMENTO\n",
      "2026-02-10 12:34:25,846 - INFO - ================================================================================\n",
      "2026-02-10 12:34:25,846 - INFO - \n",
      "[1] Carregando dados do CSV...\n",
      "2026-02-10 12:34:25,847 - INFO -     Arquivo: ../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "2026-02-10 12:34:25,875 - INFO -     ‚úì Dados carregados: (1720, 40)\n",
      "2026-02-10 12:34:25,876 - INFO -     Colunas: 40\n",
      "2026-02-10 12:34:25,877 - INFO - \n",
      "[2] Executando pipeline de processamento local...\n",
      "2026-02-10 12:34:25,965 - INFO - ‚úì Processamento conclu√≠do\n",
      "2026-02-10 12:34:25,965 - INFO -   Shape final: (1720, 41)\n",
      "2026-02-10 12:34:25,965 - INFO - \n",
      "[3] Registrando artefatos no ClearML...\n",
      "2026-02-10 12:34:25,976 - INFO - M√©tricas registradas: ['linhas_entrada', 'colunas_entrada', 'linhas_saida', 'colunas_saida', 'nas_removidos']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Iniciando pipeline de processamento BASE...\n",
      "  1Ô∏è‚É£ Aplicando substitui√ß√µes de limpeza...\n",
      "  2Ô∏è‚É£ Convertendo tipos de dados...\n",
      "  3Ô∏è‚É£ Imputando valores faltantes...\n",
      "  4Ô∏è‚É£ Criando agrupamento temporal...\n",
      "‚úÖ Pipeline BASE conclu√≠do! Shape final: (1720, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:34:39,125 - INFO - DataFrame registrado: dados_processados_sample (shape: (100, 41))\n",
      "2026-02-10 12:34:39,126 - INFO - ‚úì Artefatos registrados\n",
      "2026-02-10 12:34:39,126 - INFO - \n",
      "[4] Criando dataset ClearML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/801c2956f7704234b987beef3f9cc1e6/experiments/73e2a80ce490442eb0682b2bcbd5ff00/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/801c2956f7704234b987beef3f9cc1e6/experiments/73e2a80ce490442eb0682b2bcbd5ff00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:35:56,478 - INFO - Dataset criado: dados_processados (ID: 73e2a80ce490442eb0682b2bcbd5ff00, Projeto: Datasets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading dataset changes (1 files compressed to 49.62 KiB) to https://files.clear.ml\n",
      "File compression and upload completed: total size 49.62 KiB, 1 chunk(s) stored (average size 49.62 KiB)\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n",
      "ClearML Monitor: Reporting detected, reverting back to iteration based reporting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:37:22,800 - INFO - ‚úì Dataset criado (ID: 73e2a80ce490442eb0682b2bcbd5ff00)\n",
      "2026-02-10 12:37:22,800 - INFO - \n",
      "[5] Fechando task...\n",
      "2026-02-10 12:37:33,320 - INFO - ‚úì Task finalizada e pronta para pr√≥ximo pipeline\n",
      "2026-02-10 12:37:33,320 - INFO - \n",
      "================================================================================\n",
      "2026-02-10 12:37:33,322 - INFO - PIPELINE CONCLU√çDO COM SUCESSO\n",
      "2026-02-10 12:37:33,322 - INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Processamento\n",
    "# ============================================================================\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_processamento_clearml\n",
    "\n",
    "arquivo = '../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv'\n",
    "\n",
    "resultado = executar_pipeline_processamento_clearml(\n",
    "    caminho_csv=arquivo,\n",
    "    offline_mode=False\n",
    ")\n",
    "\n",
    "df_features = resultado['dados_processados']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8b544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_anterior_id = resultado['dataset_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e604989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:37:33,344 - INFO - Modo ONLINE: Iniciando integra√ß√£o ClearML\n",
      "2026-02-10 12:37:33,345 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-10 12:37:33,347 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-10 12:37:33,348 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-10 12:37:33,349 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-10 12:38:42,603 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:39:09,902 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:39:45,194 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:40:49,638 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.started\n",
      "2026-02-10 12:41:16,936 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.started\n",
      "2026-02-10 12:41:52,255 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.started\n",
      "2026-02-10 12:42:43,570 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.started\n",
      "2026-02-10 12:44:06,913 - WARNING - Retrying (Retry(total=233, connect=240, read=233, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=be0540b7be854c5783724ea7b04a13d8\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/be0540b7be854c5783724ea7b04a13d8/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:45:31,656 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:45:33,647 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:45:55,414 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:45:58,934 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:46:22,733 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:46:34,255 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:46:58,042 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:47:23,258 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:47:25,560 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:47:49,364 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:47:50,557 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:48:48,850 - WARNING - Retrying (Retry(total=233, connect=240, read=233, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_by_id\n",
      "2026-02-10 12:49:04,726 - INFO - Task criada: Pipeline_Features (ID: be0540b7be854c5783724ea7b04a13d8, Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS)\n",
      "2026-02-10 12:49:58,736 - INFO - ‚úì Task ClearML criada (ID: be0540b7be854c5783724ea7b04a13d8)\n",
      "2026-02-10 12:49:58,737 - INFO - ================================================================================\n",
      "2026-02-10 12:49:58,737 - INFO - PIPELINE DE ENGENHARIA DE FEATURES\n",
      "2026-02-10 12:49:58,738 - INFO - ================================================================================\n",
      "2026-02-10 12:49:58,739 - INFO - \n",
      "[1] Executando pipeline de features local...\n",
      "2026-02-10 12:49:58,740 - INFO -     Shape entrada: (1720, 41)\n",
      "2026-02-10 12:49:58,834 - INFO - ‚úì Features criadas com sucesso\n",
      "2026-02-10 12:49:58,835 - INFO -   Shape final: (1720, 78)\n",
      "2026-02-10 12:49:58,836 - INFO -   Novas colunas: 37\n",
      "2026-02-10 12:49:58,836 - INFO - \n",
      "[2] Registrando artefatos no ClearML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Iniciando pipeline de FEATURES...\n",
      "  1Ô∏è‚É£ Criando features derivadas (5 tipos)...\n",
      "  2Ô∏è‚É£ Aplicando codifica√ß√£o (label)...\n",
      "  3Ô∏è‚É£ Aplicando normaliza√ß√£o (standard)...\n",
      "‚úÖ Pipeline FEATURES conclu√≠do! Shape final: (1720, 78)\n",
      "   Novas colunas criadas: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:50:12,419 - INFO - DataFrame registrado: dados_features (shape: (1720, 78))\n",
      "2026-02-10 12:50:21,083 - INFO - Arquivo registrado: artefatos_features\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] O arquivo j√° est√° sendo usado por outro processo: 'temp_artefatos_features.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclearml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines_clearml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m executar_pipeline_features_clearml\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m resultado_features = \u001b[43mexecutar_pipeline_features_clearml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_processado\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_processado_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_anterior_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffline_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Configura√ß√µes de features\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43maplicar_codificacao\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetodo_codificacao\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriar_features_derivadas\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtipos_features_derivadas\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimc_classe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheat_index\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdew_point\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mt*u\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43maplicar_normalizacao\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetodo_normalizacao\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Arquivos_Pessoais\\PROJETOS\\tcc_mba_esalq_mlops\\notebooks\\..\\src\\clearml\\pipelines_clearml\\pipeline_features_clearml.py:177\u001b[39m, in \u001b[36mexecutar_pipeline_features_clearml\u001b[39m\u001b[34m(df_processado, dataset_processado_id, project_name, offline_mode, colunas_categoricas, aplicar_codificacao, metodo_codificacao, aplicar_normalizacao, metodo_normalizacao, criar_features_derivadas, tipos_features_derivadas)\u001b[39m\n\u001b[32m    174\u001b[39m         json.dump(artefatos_serializaveis, f, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    176\u001b[39m     registrar_arquivo(temp_artefatos, \u001b[33m\"\u001b[39m\u001b[33martefatos_features\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[43mtemp_artefatos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Artefatos registrados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(artefatos.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Registrar m√©tricas b√°sicas\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\pathlib.py:1147\u001b[39m, in \u001b[36mPath.unlink\u001b[39m\u001b[34m(self, missing_ok)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[33;03mRemove this file or link.\u001b[39;00m\n\u001b[32m   1144\u001b[39m \u001b[33;03mIf the path is a directory, use rmdir() instead.\u001b[39;00m\n\u001b[32m   1145\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_ok:\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] O arquivo j√° est√° sendo usado por outro processo: 'temp_artefatos_features.json'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:53:55,096 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:54:22,065 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:54:22,398 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:54:49,370 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:54:57,707 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:54:59,103 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:55:24,685 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:55:26,387 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:55:49,019 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:56:01,738 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:56:15,986 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:56:53,070 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n",
      "2026-02-10 12:57:12,305 - WARNING - Retrying (Retry(total=233, connect=240, read=233, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/tasks.get_all\n",
      "2026-02-10 12:57:39,278 - WARNING - Retrying (Retry(total=233, connect=240, read=233, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/events.add_batch\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE 2: ENGENHARIA DE FEATURES COM CLEARML\n",
    "# ============================================================================\n",
    "# Recebe o resultado do Pipeline 1 e aplica transforma√ß√µes de features\n",
    "# ============================================================================\n",
    "\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_features_clearml\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resultado_features = executar_pipeline_features_clearml(\n",
    "    df_processado=df_features,\n",
    "    dataset_processado_id=dataset_anterior_id,\n",
    "    offline_mode=False,\n",
    "    # Configura√ß√µes de features\n",
    "    aplicar_codificacao=True,\n",
    "    metodo_codificacao='label',\n",
    "    criar_features_derivadas=True,\n",
    "    tipos_features_derivadas=['imc', 'imc_classe', 'heat_index', 'dew_point', 't*u'],\n",
    "    aplicar_normalizacao=True,\n",
    "    metodo_normalizacao='standard'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE 3: TREINAMENTO DE MODELOS COM CLEARML\n",
    "# ============================================================================\n",
    "# Recebe o resultado do Pipeline 2 e treina modelos com PyCaret\n",
    "# ============================================================================\n",
    "\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_treinamento_clearml\n",
    "\n",
    "print(f\"  Dataset ID anterior: {resultado_features.get('dataset_id', 'N/A')}\")\n",
    "df_para_treino = resultado_features['dados_features']\n",
    "dataset_anterior_id = resultado_features.get('dataset_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resultado_treinamento = executar_pipeline_treinamento_clearml(\n",
    "    df_features=df_para_treino,\n",
    "    coluna_alvo='p1',\n",
    "    tipo_problema='regressao',\n",
    "    dataset_features_id=dataset_anterior_id,\n",
    "    offline_mode=False,\n",
    "    # Configura√ß√µes de treinamento\n",
    "    n_modelos_comparar=3,\n",
    "    otimizar_hiperparametros=True,\n",
    "    n_iter_otimizacao=10,\n",
    "    salvar_modelo_final=True,\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"../modelos\"\n",
    ")\n",
    "    \n",
    "# Melhor modelo\n",
    "melhor_modelo_nome = str(resultado_treinamento['tabela_comparacao'].index[0])\n",
    "print(f\"\\nüèÜ Melhor Modelo: {melhor_modelo_nome}\")\n",
    "        \n",
    "# M√©tricas\n",
    "print(f\"\\nüìä M√©tricas do Melhor Modelo:\")\n",
    "metricas = resultado_treinamento['metricas_melhor']\n",
    "metricas_principais = ['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']\n",
    "for nome in metricas_principais:\n",
    "    if nome in metricas:\n",
    "        valor = metricas[nome]\n",
    "        if isinstance(valor, (int, float)):\n",
    "            print(f\"  ‚Ä¢ {nome}: {valor:.4f}\")\n",
    "        \n",
    "        # Model ID\n",
    "        model_id = resultado_treinamento.get('model_id', 'N/A')\n",
    "        if model_id and model_id != 'N/A':\n",
    "            model_id_display = model_id\n",
    "        else:\n",
    "            model_id_display = \"N/A (modo offline ou erro)\"\n",
    "        \n",
    "        print(f\"\\nüíæ Modelo:\")\n",
    "        print(f\"  ‚Ä¢ Salvo em: {resultado_treinamento.get('caminho_modelo', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ ClearML ID: {model_id_display}\")\n",
    "        \n",
    "        # Compara√ß√£o de modelos\n",
    "        print(f\"\\nüìã Compara√ß√£o de Modelos (Top {len(resultado_treinamento['tabela_comparacao'])}):\")\n",
    "        print(resultado_treinamento['tabela_comparacao'][['MAE', 'MSE', 'RMSE', 'R2']].head())\n",
    "        \n",
    "        print(f\"\\nüåê Acesse: https://app.clear.ml\")\n",
    "        print(f\"  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\")\n",
    "        print(f\"  ‚Ä¢ Task: Pipeline_Treinamento\")\n",
    "        print(f\"  ‚Ä¢ Modelo: modelo_{melhor_modelo_nome}_modelo_conforto_termico\")\n",
    "        print(f\"  ‚Ä¢ Genealogia: dados_brutos ‚Üí processados ‚Üí features ‚Üí **MODELO**\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-mba-esalq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
