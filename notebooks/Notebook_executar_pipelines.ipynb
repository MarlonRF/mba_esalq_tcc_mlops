{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dd8ee5",
   "metadata": {},
   "source": [
    "## IMPORTANTE: Reiniciar Kernel\n",
    "\n",
    "Se voc√™ estiver encontrando erro `OSError: could not get source code`, **reinicie o kernel do notebook** (Ctrl+Shift+P ‚Üí \"Restart Kernel\") antes de executar as c√©lulas abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e511e6c",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bab299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados brutos: (1720, 40)\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar dados + \n",
    "import sys\n",
    "sys.path.append('..')  # Adiciona o diret√≥rio pai\n",
    "from src.utils.io.io_local import *\n",
    "from src.utils.io.io_clearml import *\n",
    "\n",
    "from src.utils.io import load_dataframe\n",
    "from config import config_custom as config\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_features import executar_pipeline_features\n",
    "from src.pipelines import treinar_pipeline_completo, treinar_rapido\n",
    "\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d914c",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bad628",
   "metadata": {},
   "source": [
    "## Pipeline processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5562b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar dados + # 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')\n",
    "\n",
    "# 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_proc = executar_pipeline_processamento(\n",
    "    df_raw,\n",
    "    config_imputacao_customizada=config.CONFIG_IMPUTACAO_CUSTOMIZADA,\n",
    "    criar_agrupamento_temporal=True,\n",
    "    nome_coluna_agrupamento='mes-ano'\n",
    ")\n",
    "print(f'Ap√≥s processamento: {df_proc.shape}')\n",
    "print(f'NAs restantes: {df_proc.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca17e6",
   "metadata": {},
   "source": [
    "## Pipeline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5eaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURES (Codifica√ß√£o + Derivadas + Normaliza√ß√£o)\n",
    "df_feat, artefatos = executar_pipeline_features(\n",
    "    df_proc,\n",
    "    # Codifica√ß√£o\n",
    "    aplicar_codificacao=True,\n",
    "    metodo_codificacao='label',           # 'label' ou 'onehot'\n",
    "    sufixo_codificacao='_cod',\n",
    "    \n",
    "    # Features derivadas\n",
    "    criar_features_derivadas=True,\n",
    "    tipos_features_derivadas=[\n",
    "        'imc',                            # √çndice de Massa Corporal\n",
    "        'imc_classe',                     # Classe do IMC\n",
    "        'heat_index',                     # √çndice de calor\n",
    "        'dew_point',                      # Ponto de orvalho\n",
    "        't*u',                            # Temperatura √ó Umidade\n",
    "    ],\n",
    "    \n",
    "    # Normaliza√ß√£o\n",
    "    aplicar_normalizacao=True,\n",
    "    metodo_normalizacao='standard',       # 'standard', 'minmax', 'robust'\n",
    "    agrupamento_normalizacao='mes-ano',   # Normalizar por grupo\n",
    "    sufixo_normalizacao='_norm',\n",
    ")\n",
    "\n",
    "print(f'Ap√≥s features: {df_feat.shape}')\n",
    "print(f'Artefatos criados: {list(artefatos.keys())}')\n",
    "df_feat.to_csv(\"../dados/resultados/dados_processados_novas_features.csv\")\n",
    "# Visualizar resultado\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55638322",
   "metadata": {},
   "source": [
    "# Pipeline de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_gerais import PARAMS_PADRAO\n",
    "\n",
    "# Definir coluna alvo e features\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "# Usar apenas features principais para teste\n",
    "features_treino = [\n",
    "    'idade', 'sexo_cod', 'peso', 'altura',\n",
    "    'tmedia', 'ur', 'vel_vento',\n",
    "]\n",
    "\n",
    "tipos_modelos ='regressao'\n",
    "\n",
    "# Filtrar features que existem no DataFrame\n",
    "features_existentes = [f for f in features_treino if f in df_feat.columns]\n",
    "print(f\"Features para treinamento: {features_existentes}\")\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_feat[features_existentes + [coluna_alvo]].dropna()\n",
    "print(f\"Dataset de treino: {df_treino.shape}\")\n",
    "\n",
    "# 4. TREINAMENTO\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ INICIANDO PIPELINE DE TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema=tipos_modelos,  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTADOS DO TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nome do melhor modelo\n",
    "nome_modelo = str(resultado['tabela_comparacao'].index[0])\n",
    "print(f\"\\n‚úì Melhor modelo: {nome_modelo}\")\n",
    "\n",
    "print(f\"\\nüìà M√©tricas principais:\")\n",
    "metricas = resultado['metricas_melhor']\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        print(f\"  ‚Ä¢ {nome}: {valor:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Modelo salvo em: {resultado.get('caminho_modelo', 'N/A')}\")\n",
    "\n",
    "print(\"\\nüìã Compara√ß√£o de modelos (top 5 m√©tricas):\")\n",
    "print(resultado['tabela_comparacao'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_10_experimentos = {}\n",
    "for i in range(10):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ü§ñ INICIANDO EXPERIMENTO {i+1}/10\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "    \n",
    "    resultados_10_experimentos[f'Experimento_{i+1}'] = resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_comparacao = [dicionario[\"tabela_comparacao\"] for dicionario in resultados_10_experimentos.values()] \n",
    "tabelas_comparacao =  [tabela.rename(columns={'Model':'Modelos','Accuracy': 'Acur√°cia', 'AUC': 'AUC', 'Recall': 'Recall', 'Prec.': 'Prec.', 'F1': 'F1'}) for tabela in tabelas_comparacao]\n",
    "serie_nomes_modelos = tabelas_comparacao[0]['Modelos']\n",
    "tabelas_comparacao = [tabela.select_dtypes(include='number') for tabela in tabelas_comparacao] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_desvio_metricas= pd.concat(tabelas_comparacao).groupby(level=0).std()\n",
    "df_media_metricas = pd.concat(tabelas_comparacao).groupby(level=0).mean()\n",
    "df_media_desvio_metricas_str = df_media_metricas.round(2).astype(str) + \" ¬± \" + df_desvio_metricas.round(2).astype(str)\n",
    "df_media_desvio_metricas_str = df_media_desvio_metricas_str.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_media_metricas = df_media_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas = df_desvio_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas.to_csv('../dados/resultados/desvio_metricas.csv')\n",
    "df_media_metricas.to_csv('../dados/resultados/media_metricas.csv')\n",
    "df_media_desvio_metricas_str.to_csv('../dados/resultados/media_e_desvio_metricas_str.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_media_desvio_metricas_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c814466",
   "metadata": {},
   "source": [
    "# ClearML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fe852",
   "metadata": {},
   "source": [
    "## Pipeline Completo - Vers√£o Local\n",
    "\n",
    "**Por que n√£o usar os decorators ClearML aqui?**\n",
    "\n",
    "Os decorators `@pipeline` e `@component` do ClearML **sempre** tentam criar a infraestrutura de pipeline (tasks, serializa√ß√£o, etc.), mesmo quando configurado para `run_locally=True`. Isso causa erros ao tentar serializar DataFrames grandes e criar tasks automaticamente.\n",
    "\n",
    "**Solu√ß√£o**: Para execu√ß√£o realmente local (sem servidor ClearML), usamos diretamente as fun√ß√µes originais dos pipelines **SEM** os decorators:\n",
    "- `executar_pipeline_processamento()` - da pasta src/pipelines\n",
    "- `treinar_pipeline_completo()` - da pasta src/treinamento\n",
    "\n",
    "Isso funciona perfeitamente porque s√£o as mesmas fun√ß√µes que voc√™ j√° usa normalmente, apenas sem a camada ClearML em cima.\n",
    "\n",
    "**Quando usar os pipelines com decorators ClearML?**\n",
    "- Quando voc√™ tem um servidor ClearML configurado\n",
    "- Quando quer enviar jobs para execu√ß√£o remota\n",
    "- Quando precisa de rastreamento completo no servidor ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PIPELINE CLEARML COMPLETO - VERS√ÉO LOCAL SIMPLIFICADA\n",
    "# =========================================================================\n",
    "# NOTA: Os decorators @pipeline do ClearML sempre tentam criar infraestrutura,\n",
    "#       mesmo com run_locally=True. Para execu√ß√£o realmente local, usamos as\n",
    "#       fun√ß√µes wrapped (sem decorators) ou reimplementamos o fluxo.\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Imports das pipelines originais (SEM decorators ClearML)\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_treinamento_unified import treinar_pipeline_completo\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PIPELINE COMPLETO - VERSAO LOCAL (SEM CLEARML)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Verificar dados\n",
    "print(f\"\\n1. Dados brutos: {df_raw.shape}\")\n",
    "\n",
    "# 2. PROCESSAMENTO\n",
    "print(f\"\\n2. Executando processamento...\")\n",
    "df_processado = executar_pipeline_processamento(df_raw)\n",
    "print(f\"   Dados processados: {df_processado.shape}\")\n",
    "\n",
    "# 3. Preparar dados para treino (remover NAs)\n",
    "print(f\"\\n3. Preparando dados para treino...\")\n",
    "print(f\"   Dados limpos: {df_treino.shape}\")\n",
    "\n",
    "# 4. TREINAMENTO\n",
    "print(f\"\\n4. Executando treinamento...\")\n",
    "print(f\"   Coluna alvo: {coluna_alvo}\")\n",
    "print(f\"   Tipo: regressao\")\n",
    "\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',\n",
    "    n_modelos_comparar=2,  # Apenas 2 modelos para teste rapido\n",
    "    otimizar_hiperparametros=False,  # Sem otimizacao para ser rapido\n",
    "    salvar_modelo_final=False,\n",
    "    nome_modelo=\"teste_clearml\"\n",
    ")\n",
    "\n",
    "# 5. RESULTADOS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "melhor_modelo = type(resultado['melhor_modelo']).__name__\n",
    "print(f\"\\nMelhor modelo: {melhor_modelo}\")\n",
    "\n",
    "print(f\"\\nMetricas:\")\n",
    "metricas = resultado['metricas_melhor']\n",
    "for nome, valor in list(metricas.items())[:5]:  # Primeiras 5 metricas\n",
    "    if isinstance(valor, (int, float)):\n",
    "        print(f\"  {nome}: {valor:.4f}\")\n",
    "\n",
    "print(f\"\\nComparacao de modelos:\")\n",
    "print(resultado['tabela_comparacao'][['MAE', 'MSE', 'RMSE', 'R2']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE CONCLUIDO COM SUCESSO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d25f1",
   "metadata": {},
   "source": [
    "## Pipeline Completo COM ClearML\n",
    "\n",
    "Agora vamos usar o ClearML **completo** para:\n",
    "1. Criar **tasks separadas** para cada etapa\n",
    "2. **Upload de datasets** (bruto e processado) como vers√µes\n",
    "3. **Registro de m√©tricas** em cada pipeline\n",
    "4. **Upload do modelo final**\n",
    "5. **Artefatos**: tabelas de compara√ß√£o, plots, etc.\n",
    "\n",
    "**Pr√©-requisitos**: \n",
    "- ClearML configurado (`clearml-init`)\n",
    "- Servidor ClearML acess√≠vel (pode ser demo.clear.ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PIPELINE COMPLETO COM CLEARML - REGISTRO TOTAL\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, OutputModel\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports dos pipelines\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_treinamento_unified import treinar_pipeline_completo\n",
    "\n",
    "# Configura√ß√£o\n",
    "PROJECT_NAME = \"conforto_termico\"\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLETO COM CLEARML - FULL TRACKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 1: UPLOAD DE DADOS BRUTOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 1] Upload de Dados Brutos para ClearML\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar dataset de dados brutos\n",
    "dataset_bruto = Dataset.create(\n",
    "    dataset_name=\"dados_brutos_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    description=\"Dados brutos de conforto termico de Santa Maria\"\n",
    ")\n",
    "\n",
    "# Salvar temporariamente para upload\n",
    "temp_path = Path(\"../dados/temp_clearml\")\n",
    "temp_path.mkdir(exist_ok=True)\n",
    "arquivo_bruto = temp_path / \"dados_brutos.csv\"\n",
    "df_raw.to_csv(arquivo_bruto, index=False)\n",
    "\n",
    "# Adicionar arquivo ao dataset\n",
    "dataset_bruto.add_files(str(arquivo_bruto))\n",
    "dataset_bruto.upload()\n",
    "dataset_bruto.finalize()\n",
    "\n",
    "print(f\"Dataset bruto criado: ID = {dataset_bruto.id}\")\n",
    "print(f\"  - Shape: {df_raw.shape}\")\n",
    "print(f\"  - Colunas: {len(df_raw.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 2: PIPELINE DE PROCESSAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 2] Pipeline de Processamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de processamento (COM FIX PARA NOTEBOOKS)\n",
    "task_processamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Processamento\",\n",
    "    task_type=Task.TaskTypes.data_processing,\n",
    "    reuse_last_task_id=False,  # N√£o reusar task anterior\n",
    "    continue_last_task=False,   # Criar nova task sempre\n",
    "    auto_connect_frameworks=False,  # Desabilita captura autom√°tica\n",
    "    auto_resource_monitoring=False  # Desabilita monitoramento de recursos\n",
    ")\n",
    "\n",
    "# Conectar dataset de entrada\n",
    "task_processamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_bruto.id,\n",
    "    \"dataset_name\": \"dados_brutos_conforto_termico\"\n",
    "})\n",
    "\n",
    "# Executar processamento\n",
    "print(\"Executando processamento...\")\n",
    "df_processado = executar_pipeline_processamento(df_raw)\n",
    "\n",
    "# Registrar m√©tricas de processamento\n",
    "task_processamento.get_logger().report_single_value(\"linhas_entrada\", df_raw.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"linhas_saida\", df_processado.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_entrada\", df_raw.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_saida\", df_processado.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"nas_removidos\", df_raw.isna().sum().sum() - df_processado.isna().sum().sum())\n",
    "\n",
    "print(f\"Dados processados: {df_processado.shape}\")\n",
    "print(f\"NAs restantes: {df_processado.isna().sum().sum()}\")\n",
    "\n",
    "# Upload de dados processados como dataset\n",
    "dataset_processado = Dataset.create(\n",
    "    dataset_name=\"dados_processados_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    parent_datasets=[dataset_bruto.id],\n",
    "    description=\"Dados processados (sem NAs, limpeza aplicada)\"\n",
    ")\n",
    "\n",
    "arquivo_processado = temp_path / \"dados_processados.csv\"\n",
    "df_processado.to_csv(arquivo_processado, index=False)\n",
    "dataset_processado.add_files(str(arquivo_processado))\n",
    "dataset_processado.upload()\n",
    "dataset_processado.finalize()\n",
    "\n",
    "print(f\"Dataset processado criado: ID = {dataset_processado.id}\")\n",
    "\n",
    "# Registrar como artefato na task\n",
    "task_processamento.upload_artifact(\n",
    "    \"dados_processados_sample\",\n",
    "    artifact_object=df_processado.head(100)  # Apenas amostra\n",
    ")\n",
    "\n",
    "task_processamento.close()\n",
    "print(\"Task de processamento finalizada!\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 3: PIPELINE DE TREINAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 3] Pipeline de Treinamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de treinamento (COM FIX PARA NOTEBOOKS)\n",
    "task_treinamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Treinamento\",\n",
    "    task_type=Task.TaskTypes.training,\n",
    "    reuse_last_task_id=False,\n",
    "    continue_last_task=False,\n",
    "    auto_connect_frameworks=False,\n",
    "    auto_resource_monitoring=False\n",
    ")\n",
    "\n",
    "# Conectar dataset processado\n",
    "task_treinamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_processado.id,\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"tipo_problema\": \"regressao\"\n",
    "})\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_processado.dropna()\n",
    "print(f\"Dados para treino: {df_treino.shape}\")\n",
    "\n",
    "# Executar treinamento\n",
    "print(\"Executando treinamento...\")\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',\n",
    "    n_modelos_comparar=3,\n",
    "    otimizar_hiperparametros=True,\n",
    "    n_iter_otimizacao=10,\n",
    "    salvar_modelo_final=True,\n",
    "    nome_modelo=\"modelo_conforto_termico_clearml\",\n",
    "    pasta_modelos=\"../modelos\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 4: REGISTRO DE M√âTRICAS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 4] Registro de M√©tricas\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "logger = task_treinamento.get_logger()\n",
    "\n",
    "# M√©tricas do melhor modelo\n",
    "metricas = resultado['metricas_melhor']\n",
    "print(\"\\nMetricas registradas:\")\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        logger.report_single_value(nome, valor)\n",
    "        print(f\"  {nome}: {valor:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 5: UPLOAD DE TABELAS E ARTEFATOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 5] Upload de Tabelas e Artefatos\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tabela de compara√ß√£o de modelos\n",
    "tabela_comparacao = resultado['tabela_comparacao']\n",
    "task_treinamento.upload_artifact(\n",
    "    \"comparacao_modelos\",\n",
    "    artifact_object=tabela_comparacao\n",
    ")\n",
    "print(\"Tabela de comparacao enviada como artefato\")\n",
    "\n",
    "# Criar gr√°fico de compara√ß√£o de modelos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "tabela_comparacao[['MAE', 'MSE', 'RMSE', 'R2']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Comparacao de Modelos - Metricas')\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Valor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Enviar gr√°fico\n",
    "logger.report_matplotlib_figure(\n",
    "    title=\"Comparacao de Modelos\",\n",
    "    series=\"Metricas\",\n",
    "    figure=fig,\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "print(\"Grafico de comparacao enviado\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 6: REGISTRO DO MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 6] Registro do Modelo\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar OutputModel\n",
    "melhor_modelo_nome = str(resultado['tabela_comparacao'].index[0])\n",
    "output_model = OutputModel(\n",
    "    task=task_treinamento,\n",
    "    name=f\"modelo_{melhor_modelo_nome}_conforto_termico\",\n",
    "    framework=\"PyCaret\"\n",
    ")\n",
    "\n",
    "# Registrar informa√ß√µes do modelo\n",
    "output_model.update_labels({\n",
    "    \"tipo\": \"regressao\",\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"melhor_modelo\": melhor_modelo_nome\n",
    "})\n",
    "\n",
    "# Adicionar m√©tricas ao modelo\n",
    "output_model.update_design(config_dict={\n",
    "    \"metricas\": {k: float(v) for k, v in metricas.items() if isinstance(v, (int, float))},\n",
    "    \"n_features\": df_treino.shape[1] - 1,\n",
    "    \"n_samples_treino\": df_treino.shape[0]\n",
    "})\n",
    "\n",
    "# Upload do arquivo do modelo\n",
    "if 'caminho_modelo' in resultado and resultado['caminho_modelo']:\n",
    "    caminho_modelo = Path(resultado['caminho_modelo'])\n",
    "    if caminho_modelo.exists():\n",
    "        output_model.update_weights(weights_filename=str(caminho_modelo))\n",
    "        print(f\"Modelo registrado: {caminho_modelo}\")\n",
    "    else:\n",
    "        print(f\"AVISO: Arquivo do modelo nao encontrado: {caminho_modelo}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 7: FINALIZA√á√ÉO E RESUMO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 7] Finalizacao\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tags na task\n",
    "task_treinamento.add_tags([\n",
    "    \"producao\",\n",
    "    melhor_modelo_nome,\n",
    "    f\"r2_{metricas.get('R2', 0):.2f}\".replace(\".\", \"_\")\n",
    "])\n",
    "\n",
    "task_treinamento.close()\n",
    "\n",
    "# Limpar arquivos tempor√°rios\n",
    "import shutil\n",
    "if temp_path.exists():\n",
    "    shutil.rmtree(temp_path)\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMO FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETO FINALIZADO - TUDO REGISTRADO NO CLEARML!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProjeto: {PROJECT_NAME}\")\n",
    "print(f\"\\nDatasets criados:\")\n",
    "print(f\"  1. Dados brutos: {dataset_bruto.id}\")\n",
    "print(f\"  2. Dados processados: {dataset_processado.id}\")\n",
    "print(f\"\\nTasks criadas:\")\n",
    "print(f\"  1. Processamento: {task_processamento.id}\")\n",
    "print(f\"  2. Treinamento: {task_treinamento.id}\")\n",
    "print(f\"\\nModelo registrado:\")\n",
    "print(f\"  - Nome: modelo_{melhor_modelo_nome}_conforto_termico\")\n",
    "print(f\"  - Melhor modelo: {melhor_modelo_nome}\")\n",
    "print(f\"  - R2: {metricas.get('R2', 0):.4f}\")\n",
    "print(f\"  - MAE: {metricas.get('MAE', 0):.4f}\")\n",
    "print(\"\\nAcesse o ClearML UI para visualizar todos os resultados!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaf991",
   "metadata": {},
   "source": [
    "## Visualizar Resultados no ClearML\n",
    "\n",
    "Ap√≥s executar a c√©lula acima, acesse a interface web do ClearML:\n",
    "- **Local**: http://localhost:8080\n",
    "- **Demo**: https://app.clear.ml\n",
    "\n",
    "### O que voc√™ vai encontrar:\n",
    "\n",
    "**üìä No Projeto \"conforto_termico\":**\n",
    "- 2 Tasks (Processamento + Treinamento)\n",
    "- 2 Datasets (Bruto + Processado) com genealogia\n",
    "- 1 Modelo registrado com m√©tricas\n",
    "\n",
    "**üìà M√©tricas rastreadas:**\n",
    "- Todas as m√©tricas de regress√£o (MAE, MSE, RMSE, R2, etc.)\n",
    "- Compara√ß√£o entre modelos\n",
    "- Gr√°ficos de compara√ß√£o\n",
    "\n",
    "**üì¶ Artefatos salvos:**\n",
    "- Tabela de compara√ß√£o de modelos\n",
    "- Sample dos dados processados\n",
    "- Gr√°fico de m√©tricas\n",
    "\n",
    "**ü§ñ Modelo:**\n",
    "- Arquivo .pkl do modelo treinado\n",
    "- Labels e configura√ß√µes\n",
    "- Genealogia completa (dados ‚Üí processamento ‚Üí treinamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba750975",
   "metadata": {},
   "source": [
    "## [OPCIONAL] Buscar e Reusar Recursos do ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# REUSAR RECURSOS DO CLEARML (Datasets, Modelos, Tasks)\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, Model\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_NAME = \"conforto_termico_def\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUSCANDO RECURSOS NO CLEARML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LISTAR DATASETS DISPON√çVEIS\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Datasets disponiveis no projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "datasets = Dataset.list_datasets(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    partial_name=\"conforto_termico\"\n",
    ")\n",
    "\n",
    "for i, ds in enumerate(datasets, 1):\n",
    "    print(f\"\\n{i}. {ds.name}\")\n",
    "    print(f\"   ID: {ds.id}\")\n",
    "    print(f\"   Versao: {ds.version}\")\n",
    "    print(f\"   Criado: {ds.created}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. BAIXAR DATASET ESPEC√çFICO\n",
    "# ============================================================================\n",
    "print(\"\\n[2] Baixando dataset processado:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Buscar o dataset mais recente de dados processados\n",
    "dataset_processado = Dataset.get(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    dataset_name=\"dados_processados_conforto_termico\"\n",
    ")\n",
    "\n",
    "# Baixar para pasta local\n",
    "local_path = dataset_processado.get_local_copy()\n",
    "print(f\"Dataset baixado para: {local_path}\")\n",
    "\n",
    "# Carregar dados\n",
    "arquivos = list(Path(local_path).glob(\"*.csv\"))\n",
    "if arquivos:\n",
    "    df_downloaded = pd.read_csv(arquivos[0])\n",
    "    print(f\"Dados carregados: {df_downloaded.shape}\")\n",
    "    print(df_downloaded.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LISTAR TASKS DO PROJETO\n",
    "# ============================================================================\n",
    "print(\"\\n[3] Tasks do projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "tasks = Task.get_tasks(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=None  # Todas as tasks\n",
    ")\n",
    "\n",
    "for i, t in enumerate(tasks[:5], 1):  # Primeiras 5\n",
    "    print(f\"\\n{i}. {t.name}\")\n",
    "    print(f\"   ID: {t.id}\")\n",
    "    print(f\"   Status: {t.status}\")\n",
    "    print(f\"   Tipo: {t.task_type}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUSCAR MODELOS REGISTRADOS\n",
    "# ============================================================================\n",
    "print(\"\\n[4] Modelos registrados:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "modelos = Model.query_models(\n",
    "    project_name=PROJECT_NAME,\n",
    "    model_name=None  # Todos os modelos\n",
    ")\n",
    "\n",
    "for i, modelo in enumerate(modelos, 1):\n",
    "    print(f\"\\n{i}. {modelo.name}\")\n",
    "    print(f\"   ID: {modelo.id}\")\n",
    "    print(f\"   Framework: {modelo.framework}\")\n",
    "    print(f\"   Labels: {modelo.labels}\")\n",
    "    \n",
    "    # Baixar modelo se quiser\n",
    "    # modelo_local = modelo.get_local_copy()\n",
    "    # print(f\"   Download: {modelo_local}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CLONAR E REUSAR TASK\n",
    "# ============================================================================\n",
    "print(\"\\n[5] Exemplo: Clonar uma task existente\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if tasks:\n",
    "    task_original = tasks[0]\n",
    "    print(f\"Clonando task: {task_original.name}\")\n",
    "    \n",
    "    # Clonar task (cria uma c√≥pia)\n",
    "    task_clonada = Task.clone(\n",
    "        source_task=task_original.id,\n",
    "        name=f\"{task_original.name}_clonada\",\n",
    "        project=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Task clonada criada: {task_clonada.id}\")\n",
    "    print(\"Voc√™ pode executar esta task clonada com configuracoes diferentes!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSCA DE RECURSOS CONCLUIDA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab2d29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Resumo: ClearML vs Local\n",
    "\n",
    "| Recurso | **Local (sem ClearML)** | **Com ClearML** |\n",
    "|---------|------------------------|-----------------|\n",
    "| **Execu√ß√£o** | R√°pida, sem overhead | Overhead de registro, mas rastre√°vel |\n",
    "| **Datasets** | Arquivos locais | Versionados no servidor, com genealogia |\n",
    "| **Modelos** | Apenas arquivos .pkl | Registrados com metadados, m√©tricas, tags |\n",
    "| **M√©tricas** | Apenas print no console | Dashboard visual, compar√°vel |\n",
    "| **Reprodutibilidade** | Manual (c√≥digo) | Autom√°tica (par√¢metros, ambiente, c√≥digo) |\n",
    "| **Colabora√ß√£o** | Dif√≠cil | F√°cil (servidor compartilhado) |\n",
    "| **Hist√≥rico** | N√£o rastreado | Completo (todas as execu√ß√µes) |\n",
    "\n",
    "### üí° Quando usar cada um:\n",
    "\n",
    "**Use LOCAL (c√©lula anterior):**\n",
    "- Desenvolvimento r√°pido\n",
    "- Testes locais\n",
    "- Notebooks explorat√≥rios\n",
    "- Quando n√£o tem servidor ClearML\n",
    "\n",
    "**Use ClearML (esta se√ß√£o):**\n",
    "- Produ√ß√£o\n",
    "- Experimentos para comparar\n",
    "- Trabalho em equipe\n",
    "- Quando precisa rastrear tudo\n",
    "- Deploy de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3acffc8",
   "metadata": {},
   "source": [
    "## Pipeline Completo COM Decorators (@PipelineDecorator)\n",
    "\n",
    "Esta √© a solu√ß√£o **completa** usando os decorators oficiais do ClearML:\n",
    "- `@PipelineDecorator.pipeline` - Define o pipeline completo\n",
    "- `@PipelineDecorator.component` - Define cada componente do pipeline\n",
    "\n",
    "### üéØ O que este pipeline faz:\n",
    "\n",
    "1. **Cria projeto automaticamente** no ClearML (se n√£o existir)\n",
    "2. **Registra 3 datasets versionados**:\n",
    "   - Dados brutos\n",
    "   - Dados processados\n",
    "   - Dados com features\n",
    "3. **Cria 5 tasks separadas** (uma para cada componente)\n",
    "4. **Registra modelo** com m√©trica principal (R¬≤) para decis√£o de modelo vigente\n",
    "5. **Artefatos**: tabelas, gr√°ficos, encoders, scalers\n",
    "\n",
    "### üìù Modos de execu√ß√£o:\n",
    "\n",
    "- **Local**: `run_locally=True` - executa no notebook, mas registra tudo no ClearML\n",
    "- **Remoto**: `run_locally=False` - envia para fila de execu√ß√£o remota\n",
    "\n",
    "### ‚öôÔ∏è Arquivo criado:\n",
    "\n",
    "[src/clearml/pipeline_completo_decorators.py](../src/clearml/pipeline_completo_decorators.py)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANTE: Problema de Cache\n",
    "\n",
    "Se voc√™ receber o erro `OSError: could not get source code`:\n",
    "\n",
    "**SOLU√á√ÉO**: Reinicie o kernel do notebook:\n",
    "1. Pressione `Ctrl+Shift+P` (ou `Cmd+Shift+P` no Mac)\n",
    "2. Digite \"Restart Kernel\"\n",
    "3. Selecione \"Jupyter: Restart Kernel\"\n",
    "4. Execute a c√©lula abaixo novamente\n",
    "\n",
    "Isso acontece porque o Python mant√©m em cache o c√≥digo fonte dos m√≥dulos importados. Quando modificamos o arquivo depois de import√°-lo, o cache fica desatualizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTANTE: EXECUTE ESTA CELULA APENAS UMA VEZ\n",
    "# Se der erro de \"could not get source code\", reinicie o kernel manualmente:\n",
    "# Ctrl+Shift+P -> \"Restart Kernel\" e execute esta celula novamente\n",
    "# ============================================================================\n",
    "\n",
    "from src.clearml.pipeline_completo_decorators import executar_pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "# Usar caminho absoluto para evitar problemas\n",
    "CAMINHO_DADOS = str(Path('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv').resolve())\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXECUTANDO PIPELINE COMPLETO COM @PipelineDecorator\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nArquivo CSV: {CAMINHO_DADOS}\")\n",
    "print(f\"Arquivo existe: {Path(CAMINHO_DADOS).exists()}\")\n",
    "print(\"Modo: EXECUCAO LOCAL (com tracking ClearML)\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    resultado = executar_pipeline(\n",
    "        caminho_csv=CAMINHO_DADOS,\n",
    "        run_locally=True  # True = local, False = remoto (precisa de fila configurada)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESUMO FINAL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if resultado:\n",
    "        print(f\"\\nPipeline concluido com sucesso!\")\n",
    "        print(f\"\\nDatasets criados:\")\n",
    "        for key, value in resultado.items():\n",
    "            if 'dataset' in key:\n",
    "                print(f\"  - {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nModelo:\")\n",
    "        if 'model' in resultado:\n",
    "            print(f\"  - ID: {resultado['model']}\")\n",
    "        \n",
    "        print(f\"\\nMetricas:\")\n",
    "        if 'metricas' in resultado:\n",
    "            for metric, value in resultado['metricas'].items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"  - {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nPipeline falhou\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ERRO NO PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nErro: {type(e).__name__}\")\n",
    "    print(f\"Mensagem: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nTraceback completo:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990bc2e",
   "metadata": {},
   "source": [
    "### üîç Como funciona internamente:\n",
    "\n",
    "O pipeline usa **5 componentes** encadeados:\n",
    "\n",
    "```python\n",
    "@PipelineDecorator.component - component_upload_dados_brutos\n",
    "    ‚Üì (dataset_bruto_id)\n",
    "@PipelineDecorator.component - component_pipeline_processamento\n",
    "    ‚Üì (dataset_processado_id)\n",
    "@PipelineDecorator.component - component_pipeline_features\n",
    "    ‚Üì (dataset_features_id)\n",
    "@PipelineDecorator.component - component_pipeline_treinamento\n",
    "    ‚Üì (resultado_treinamento, caminho_modelo)\n",
    "@PipelineDecorator.component - component_registrar_modelo\n",
    "    ‚Üì (model_id)\n",
    "@PipelineDecorator.pipeline - pipeline_completo_clearml\n",
    "    ‚Üì (resumo completo)\n",
    "```\n",
    "\n",
    "Cada componente:\n",
    "- Cria sua pr√≥pria Task no ClearML\n",
    "- Pode ser executado independentemente\n",
    "- Pode ter cache (exceto treinamento)\n",
    "- Retorna valores serializ√°veis para o pr√≥ximo\n",
    "\n",
    "### üìå M√©trica Principal para Decis√£o de Modelo Vigente\n",
    "\n",
    "O pipeline registra **R¬≤** como m√©trica principal no modelo. Esta m√©trica √© usada para:\n",
    "- Comparar modelos entre experimentos\n",
    "- Decidir qual modelo deve ser promovido para produ√ß√£o\n",
    "- Configur√°vel em `METRICA_PRINCIPAL` no arquivo\n",
    "\n",
    "Para classifica√ß√£o, voc√™ pode mudar para: `METRICA_PRINCIPAL = \"Accuracy\"` ou `\"AUC\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clearml.executar_pipelines import executar_pipelines_completo\n",
    "\n",
    "resultado = executar_pipelines_completo(\n",
    "    caminho_csv=\"dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\",\n",
    "    coluna_alvo=\"p1\",\n",
    "    n_modelos=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8258d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGN√ìSTICO: Verificar Configura√ß√£o Antes de Executar Pipelines\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGN√ìSTICO PR√â-PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Verificar arquivo de dados\n",
    "arquivo_dados = Path('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f\"\\n1. Arquivo de dados:\")\n",
    "print(f\"   Caminho: {arquivo_dados.resolve()}\")\n",
    "print(f\"   Existe: {arquivo_dados.exists()}\")\n",
    "if arquivo_dados.exists():\n",
    "    print(f\"   Tamanho: {arquivo_dados.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# 2. Verificar configura√ß√£o ClearML\n",
    "print(f\"\\n2. ClearML:\")\n",
    "try:\n",
    "    from clearml import Task\n",
    "    # Tentar criar uma task de teste (sem inicializar realmente)\n",
    "    print(f\"   ‚úì ClearML instalado\")\n",
    "    \n",
    "    # Verificar arquivo de config\n",
    "    clearml_conf = Path.home() / 'clearml.conf'\n",
    "    print(f\"   Config: {clearml_conf}\")\n",
    "    print(f\"   Config existe: {clearml_conf.exists()}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"   ‚úó ClearML n√£o instalado: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö† Aviso: {e}\")\n",
    "\n",
    "# 3. Verificar m√≥dulos do projeto\n",
    "print(f\"\\n3. M√≥dulos do projeto:\")\n",
    "try:\n",
    "    from src.clearml.pipeline_01_processamento import pipeline_processamento\n",
    "    print(f\"   ‚úì pipeline_01_processamento importado\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚úó Erro ao importar pipeline_01_processamento: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "    print(f\"   ‚úì executar_pipeline_processamento importado\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚úó Erro ao importar executar_pipeline_processamento: {e}\")\n",
    "\n",
    "# 4. Verificar Python path\n",
    "print(f\"\\n4. Python path:\")\n",
    "print(f\"   Diret√≥rio atual: {Path.cwd()}\")\n",
    "print(f\"   '..' no sys.path: {'..' in sys.path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Se tudo estiver ‚úì, voc√™ pode executar os pipelines ClearML\")\n",
    "print(\"Se houver ‚úó, corrija os problemas antes de continuar\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840244b",
   "metadata": {},
   "source": [
    "### üîß Solu√ß√£o Aplicada: PipelineDecorator vs Task.init()\n",
    "\n",
    "#### ‚ùå **Problema Identificado**: `ValueError: Pipeline step \"processar_dados\" ... failed`\n",
    "\n",
    "**Causa Real**: O uso de `@PipelineDecorator` cria tasks que s√£o **enviadas para execu√ß√£o remota** no servidor ClearML, mesmo quando voc√™ executa localmente. Essas tasks remotas falham porque:\n",
    "\n",
    "1. ‚ùå O arquivo CSV est√° na sua m√°quina, n√£o no servidor\n",
    "2. ‚ùå O ambiente remoto pode n√£o ter as depend√™ncias instaladas\n",
    "3. ‚ùå O c√≥digo das suas fun√ß√µes n√£o √© transferido corretamente\n",
    "\n",
    "#### ‚úÖ **Solu√ß√£o Implementada**: Usar `Task.init()` diretamente\n",
    "\n",
    "Mudamos de:\n",
    "```python\n",
    "# ‚ùå PROBLEM√ÅTICO - cria tasks remotas\n",
    "@PipelineDecorator.pipeline(...)\n",
    "@PipelineDecorator.component(...)\n",
    "def processar_dados(...):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Para:\n",
    "```python\n",
    "# ‚úÖ FUNCIONA - executa localmente com tracking\n",
    "def pipeline_processamento(...):\n",
    "    task = Task.init(...)  # Cria task local\n",
    "    # ... seu c√≥digo aqui ...\n",
    "    task.close()\n",
    "```\n",
    "\n",
    "**Vantagens**:\n",
    "- ‚úÖ Executa **localmente** (acessa seus arquivos CSV)\n",
    "- ‚úÖ **Registra tudo** no servidor ClearML (datasets, m√©tricas, artefatos)\n",
    "- ‚úÖ N√£o precisa configurar agentes remotos\n",
    "- ‚úÖ Mais controle sobre a execu√ß√£o\n",
    "\n",
    "**Quando usar cada abordagem**:\n",
    "- `Task.init()` ‚Üí Desenvolvimento, notebooks, execu√ß√£o local com tracking\n",
    "- `@PipelineDecorator` ‚Üí Produ√ß√£o com agentes ClearML configurados e dados no servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0411ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Pipeline 1: Processamento de Dados\n",
      "Modo: EXECU√á√ÉO LOCAL com tracking ClearML\n",
      "================================================================================\n",
      "Arquivo: ../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 19:06:37,331 - WARNING - Retrying (Retry(total=237, connect=240, read=237, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/projects.get_all\n",
      "2026-02-09 19:07:04,628 - WARNING - Retrying (Retry(total=236, connect=240, read=236, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/projects.get_all\n",
      "2026-02-09 19:07:39,962 - WARNING - Retrying (Retry(total=235, connect=240, read=235, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/projects.get_all\n",
      "2026-02-09 19:08:31,254 - WARNING - Retrying (Retry(total=234, connect=240, read=234, redirect=240, status=240)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /v2.23/projects.get_all\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Pipeline 1: Processamento de Dados (VERS√ÉO CORRIGIDA)\n",
    "# ============================================================================\n",
    "# MUDAN√áA: Agora usa Task.init() em vez de @PipelineDecorator\n",
    "# MOTIVO: Decorators enviam tasks para servidor remoto, causando falhas\n",
    "# SOLU√á√ÉO: Execu√ß√£o local com tracking ClearML completo\n",
    "# ============================================================================\n",
    "\n",
    "from src.clearml.pipeline_01_processamento import pipeline_processamento\n",
    "\n",
    "# Caminho do arquivo (relativo ao notebook)\n",
    "arquivo = '../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Pipeline 1: Processamento de Dados\")\n",
    "print(\"Modo: EXECU√á√ÉO LOCAL com tracking ClearML\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Arquivo: {arquivo}\\n\")\n",
    "\n",
    "try:\n",
    "    # Executar pipeline de processamento\n",
    "    # Agora executa LOCALMENTE e registra no ClearML\n",
    "    r1 = pipeline_processamento(arquivo)\n",
    "    \n",
    "    print(\"\\n‚úì Pipeline 1 conclu√≠do com sucesso!\")\n",
    "    print(f\"\\nüì¶ Resultados:\")\n",
    "    print(f\"   Dataset ID: {r1['dataset_processado_id']}\")\n",
    "    print(f\"   Shape: {r1['shape']}\")\n",
    "    print(f\"   NAs removidos: {r1['nas_removidos']}\")\n",
    "    print(\"\\n‚úì Acesse o ClearML UI para ver m√©tricas, dataset e artefatos!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Erro no Pipeline 1:\")\n",
    "    print(f\"  Tipo: {type(e).__name__}\")\n",
    "    print(f\"  Mensagem: {str(e)}\")\n",
    "    print(\"\\nüí° Dicas:\")\n",
    "    print(\"  ‚Ä¢ Verifique se o arquivo CSV existe\")\n",
    "    print(\"  ‚Ä¢ Verifique a conex√£o com o servidor ClearML\")\n",
    "    print(\"  ‚Ä¢ Veja os logs completos acima para mais detalhes\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21465254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline 2 (usa ID do pipeline 1)\n",
    "from src.clearml.pipeline_02_features import pipeline_features\n",
    "r2 = pipeline_features(dataset_processado_id=r1['dataset_processado_id'])\n",
    "\n",
    "# Pipeline 3 (usa ID do pipeline 2)\n",
    "from src.clearml.pipeline_03_treinamento import pipeline_treinamento\n",
    "r3 = pipeline_treinamento(dataset_features_id=r2['dataset_features_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-mba-esalq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
