{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dd8ee5",
   "metadata": {},
   "source": [
    "## IMPORTANTE: Reiniciar Kernel\n",
    "\n",
    "Se voc√™ estiver encontrando erro `OSError: could not get source code`, **reinicie o kernel do notebook** (Ctrl+Shift+P ‚Üí \"Restart Kernel\") antes de executar as c√©lulas abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e511e6c",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bab299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados brutos: (1720, 40)\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar dados + \n",
    "import sys\n",
    "sys.path.append('..')  # Adiciona o diret√≥rio pai\n",
    "from src.utils.io.io_local import *\n",
    "from src.utils.io.io_clearml import *\n",
    "\n",
    "from src.utils.io import load_dataframe\n",
    "from config import config_custom as config\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_features import executar_pipeline_features\n",
    "from src.pipelines import treinar_pipeline_completo, treinar_rapido\n",
    "\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d914c",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bad628",
   "metadata": {},
   "source": [
    "## Pipeline processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5562b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados brutos: (1720, 40)\n",
      "üîÑ Iniciando pipeline de processamento BASE...\n",
      "  1Ô∏è‚É£ Aplicando substitui√ß√µes de limpeza...\n",
      "  2Ô∏è‚É£ Convertendo tipos de dados...\n",
      "  3Ô∏è‚É£ Imputando valores faltantes...\n",
      "  4Ô∏è‚É£ Criando agrupamento temporal...\n",
      "‚úÖ Pipeline BASE conclu√≠do! Shape final: (1720, 41)\n",
      "Ap√≥s processamento: (1720, 41)\n",
      "NAs restantes: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Carregar dados + # 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_raw = load_dataframe('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "print(f'Dados brutos: {df_raw.shape}')\n",
    "\n",
    "# 2. PROCESSAMENTO (Limpeza + Imputa√ß√£o)\n",
    "df_proc = executar_pipeline_processamento(\n",
    "    df_raw,\n",
    "    config_imputacao_customizada=config.CONFIG_IMPUTACAO_CUSTOMIZADA,\n",
    "    criar_agrupamento_temporal=True,\n",
    "    nome_coluna_agrupamento='mes-ano'\n",
    ")\n",
    "print(f'Ap√≥s processamento: {df_proc.shape}')\n",
    "print(f'NAs restantes: {df_proc.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca17e6",
   "metadata": {},
   "source": [
    "## Pipeline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5eaf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Iniciando pipeline de FEATURES...\n",
      "  1Ô∏è‚É£ Criando features derivadas (5 tipos)...\n",
      "  2Ô∏è‚É£ Aplicando codifica√ß√£o (label)...\n",
      "  3Ô∏è‚É£ Aplicando normaliza√ß√£o (standard)...\n",
      "‚úÖ Pipeline FEATURES conclu√≠do! Shape final: (1720, 78)\n",
      "   Novas colunas criadas: 37\n",
      "Ap√≥s features: (1720, 78)\n",
      "Artefatos criados: ['mapeamentos_codificacao', 'colunas_normalizadas', 'artefatos_codificacao', 'artefatos_normalizacao']\n",
      "  2Ô∏è‚É£ Aplicando codifica√ß√£o (label)...\n",
      "  3Ô∏è‚É£ Aplicando normaliza√ß√£o (standard)...\n",
      "‚úÖ Pipeline FEATURES conclu√≠do! Shape final: (1720, 78)\n",
      "   Novas colunas criadas: 37\n",
      "Ap√≥s features: (1720, 78)\n",
      "Artefatos criados: ['mapeamentos_codificacao', 'colunas_normalizadas', 'artefatos_codificacao', 'artefatos_normalizacao']\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\dados\\resultados'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAp√≥s features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_feat.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mArtefatos criados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(artefatos.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mdf_feat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../dados/resultados/dados_processados_novas_features.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Visualizar resultado\u001b[39;00m\n\u001b[32m     30\u001b[39m df_feat.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3891\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3893\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3894\u001b[39m     frame=df,\n\u001b[32m   3895\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3899\u001b[39m     decimal=decimal,\n\u001b[32m   3900\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3905\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3907\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m   1131\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1133\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m   1134\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m   1135\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1150\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1151\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1155\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    257\u001b[39m         handles.handle,\n\u001b[32m    258\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    264\u001b[39m     )\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:739\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    743\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:604\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    602\u001b[39m parent = Path(path).parent\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '..\\dados\\resultados'"
     ]
    }
   ],
   "source": [
    "# 3. FEATURES (Codifica√ß√£o + Derivadas + Normaliza√ß√£o)\n",
    "df_feat, artefatos = executar_pipeline_features(\n",
    "    df_proc,\n",
    "    # Codifica√ß√£o\n",
    "    aplicar_codificacao=True,\n",
    "    metodo_codificacao='label',           # 'label' ou 'onehot'\n",
    "    sufixo_codificacao='_cod',\n",
    "    \n",
    "    # Features derivadas\n",
    "    criar_features_derivadas=True,\n",
    "    tipos_features_derivadas=[\n",
    "        'imc',                            # √çndice de Massa Corporal\n",
    "        'imc_classe',                     # Classe do IMC\n",
    "        'heat_index',                     # √çndice de calor\n",
    "        'dew_point',                      # Ponto de orvalho\n",
    "        't*u',                            # Temperatura √ó Umidade\n",
    "    ],\n",
    "    \n",
    "    # Normaliza√ß√£o\n",
    "    aplicar_normalizacao=True,\n",
    "    metodo_normalizacao='standard',       # 'standard', 'minmax', 'robust'\n",
    "    agrupamento_normalizacao='mes-ano',   # Normalizar por grupo\n",
    "    sufixo_normalizacao='_norm',\n",
    ")\n",
    "\n",
    "print(f'Ap√≥s features: {df_feat.shape}')\n",
    "print(f'Artefatos criados: {list(artefatos.keys())}')\n",
    "df_feat.to_csv(\"../dados/resultados/dados_processados_novas_features.csv\")\n",
    "# Visualizar resultado\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55638322",
   "metadata": {},
   "source": [
    "# Pipeline de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_gerais import PARAMS_PADRAO\n",
    "\n",
    "# Definir coluna alvo e features\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "# Usar apenas features principais para teste\n",
    "features_treino = [\n",
    "    'idade', 'sexo_cod', 'peso', 'altura',\n",
    "    'tmedia', 'ur', 'vel_vento',\n",
    "]\n",
    "\n",
    "tipos_modelos ='regressao'\n",
    "\n",
    "# Filtrar features que existem no DataFrame\n",
    "features_existentes = [f for f in features_treino if f in df_feat.columns]\n",
    "print(f\"Features para treinamento: {features_existentes}\")\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_feat[features_existentes + [coluna_alvo]].dropna()\n",
    "print(f\"Dataset de treino: {df_treino.shape}\")\n",
    "\n",
    "# 4. TREINAMENTO\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ INICIANDO PIPELINE DE TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema=tipos_modelos,  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "\n",
    "# Visualizar resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESULTADOS DO TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nome do melhor modelo\n",
    "nome_modelo = str(resultado['tabela_comparacao'].index[0])\n",
    "print(f\"\\n‚úì Melhor modelo: {nome_modelo}\")\n",
    "\n",
    "print(f\"\\nüìà M√©tricas principais:\")\n",
    "metricas = resultado['metricas_melhor']\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        print(f\"  ‚Ä¢ {nome}: {valor:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Modelo salvo em: {resultado.get('caminho_modelo', 'N/A')}\")\n",
    "\n",
    "print(\"\\nüìã Compara√ß√£o de modelos (top 5 m√©tricas):\")\n",
    "print(resultado['tabela_comparacao'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_10_experimentos = {}\n",
    "for i in range(10):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ü§ñ INICIANDO EXPERIMENTO {i+1}/10\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',  # 'classificacao' ou 'regressao'\n",
    "    params_setup=PARAMS_PADRAO,\n",
    "    n_modelos_comparar=3,           # Testar top 3 modelos\n",
    "    otimizar_hiperparametros=True,         # Otimizar hiperpar√¢metros\n",
    "    n_iter_otimizacao=10,  # 10 itera√ß√µes de otimiza√ß√£o\n",
    "    salvar_modelo_final=True,           # Salvar modelo\n",
    "    nome_modelo=\"modelo_conforto_termico\",\n",
    "    pasta_modelos=\"modelos\"\n",
    ")\n",
    "    \n",
    "    resultados_10_experimentos[f'Experimento_{i+1}'] = resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabelas_comparacao = [dicionario[\"tabela_comparacao\"] for dicionario in resultados_10_experimentos.values()] \n",
    "tabelas_comparacao =  [tabela.rename(columns={'Model':'Modelos','Accuracy': 'Acur√°cia', 'AUC': 'AUC', 'Recall': 'Recall', 'Prec.': 'Prec.', 'F1': 'F1'}) for tabela in tabelas_comparacao]\n",
    "serie_nomes_modelos = tabelas_comparacao[0]['Modelos']\n",
    "tabelas_comparacao = [tabela.select_dtypes(include='number') for tabela in tabelas_comparacao] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_desvio_metricas= pd.concat(tabelas_comparacao).groupby(level=0).std()\n",
    "df_media_metricas = pd.concat(tabelas_comparacao).groupby(level=0).mean()\n",
    "df_media_desvio_metricas_str = df_media_metricas.round(2).astype(str) + \" ¬± \" + df_desvio_metricas.round(2).astype(str)\n",
    "df_media_desvio_metricas_str = df_media_desvio_metricas_str.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_media_metricas = df_media_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas = df_desvio_metricas.merge(serie_nomes_modelos, left_index=True, right_index=True)\n",
    "df_desvio_metricas.to_csv('../dados/resultados/desvio_metricas.csv')\n",
    "df_media_metricas.to_csv('../dados/resultados/media_metricas.csv')\n",
    "df_media_desvio_metricas_str.to_csv('../dados/resultados/media_e_desvio_metricas_str.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_media_desvio_metricas_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c814466",
   "metadata": {},
   "source": [
    "# ClearML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fe852",
   "metadata": {},
   "source": [
    "## Pipeline Completo - Vers√£o Local\n",
    "\n",
    "**Por que n√£o usar os decorators ClearML aqui?**\n",
    "\n",
    "Os decorators `@pipeline` e `@component` do ClearML **sempre** tentam criar a infraestrutura de pipeline (tasks, serializa√ß√£o, etc.), mesmo quando configurado para `run_locally=True`. Isso causa erros ao tentar serializar DataFrames grandes e criar tasks automaticamente.\n",
    "\n",
    "**Solu√ß√£o**: Para execu√ß√£o realmente local (sem servidor ClearML), usamos diretamente as fun√ß√µes originais dos pipelines **SEM** os decorators:\n",
    "- `executar_pipeline_processamento()` - da pasta src/pipelines\n",
    "- `treinar_pipeline_completo()` - da pasta src/treinamento\n",
    "\n",
    "Isso funciona perfeitamente porque s√£o as mesmas fun√ß√µes que voc√™ j√° usa normalmente, apenas sem a camada ClearML em cima.\n",
    "\n",
    "**Quando usar os pipelines com decorators ClearML?**\n",
    "- Quando voc√™ tem um servidor ClearML configurado\n",
    "- Quando quer enviar jobs para execu√ß√£o remota\n",
    "- Quando precisa de rastreamento completo no servidor ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PIPELINE COMPLETO COM CLEARML - REGISTRO TOTAL\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, OutputModel\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Imports dos pipelines\n",
    "from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "from src.pipelines.pipeline_treinamento_unified import treinar_pipeline_completo\n",
    "\n",
    "# Configura√ß√£o\n",
    "PROJECT_NAME = \"conforto_termico\"\n",
    "coluna_alvo = 'p1'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLETO COM CLEARML - FULL TRACKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 1: UPLOAD DE DADOS BRUTOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 1] Upload de Dados Brutos para ClearML\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar dataset de dados brutos\n",
    "dataset_bruto = Dataset.create(\n",
    "    dataset_name=\"dados_brutos_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    description=\"Dados brutos de conforto termico de Santa Maria\"\n",
    ")\n",
    "\n",
    "# Salvar temporariamente para upload\n",
    "temp_path = Path(\"../dados/temp_clearml\")\n",
    "temp_path.mkdir(exist_ok=True)\n",
    "arquivo_bruto = temp_path / \"dados_brutos.csv\"\n",
    "df_raw.to_csv(arquivo_bruto, index=False)\n",
    "\n",
    "# Adicionar arquivo ao dataset\n",
    "dataset_bruto.add_files(str(arquivo_bruto))\n",
    "dataset_bruto.upload()\n",
    "dataset_bruto.finalize()\n",
    "\n",
    "print(f\"Dataset bruto criado: ID = {dataset_bruto.id}\")\n",
    "print(f\"  - Shape: {df_raw.shape}\")\n",
    "print(f\"  - Colunas: {len(df_raw.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 2: PIPELINE DE PROCESSAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 2] Pipeline de Processamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de processamento (COM FIX PARA NOTEBOOKS)\n",
    "task_processamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Processamento\",\n",
    "    task_type=Task.TaskTypes.data_processing,\n",
    "    reuse_last_task_id=False,  # N√£o reusar task anterior\n",
    "    continue_last_task=False,   # Criar nova task sempre\n",
    "    auto_connect_frameworks=False,  # Desabilita captura autom√°tica\n",
    "    auto_resource_monitoring=False  # Desabilita monitoramento de recursos\n",
    ")\n",
    "\n",
    "# Conectar dataset de entrada\n",
    "task_processamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_bruto.id,\n",
    "    \"dataset_name\": \"dados_brutos_conforto_termico\"\n",
    "})\n",
    "\n",
    "# Executar processamento\n",
    "print(\"Executando processamento...\")\n",
    "df_processado = executar_pipeline_processamento(df_raw)\n",
    "\n",
    "# Registrar m√©tricas de processamento\n",
    "task_processamento.get_logger().report_single_value(\"linhas_entrada\", df_raw.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"linhas_saida\", df_processado.shape[0])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_entrada\", df_raw.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"colunas_saida\", df_processado.shape[1])\n",
    "task_processamento.get_logger().report_single_value(\"nas_removidos\", df_raw.isna().sum().sum() - df_processado.isna().sum().sum())\n",
    "\n",
    "print(f\"Dados processados: {df_processado.shape}\")\n",
    "print(f\"NAs restantes: {df_processado.isna().sum().sum()}\")\n",
    "\n",
    "# Upload de dados processados como dataset\n",
    "dataset_processado = Dataset.create(\n",
    "    dataset_name=\"dados_processados_conforto_termico\",\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    parent_datasets=[dataset_bruto.id],\n",
    "    description=\"Dados processados (sem NAs, limpeza aplicada)\"\n",
    ")\n",
    "\n",
    "arquivo_processado = temp_path / \"dados_processados.csv\"\n",
    "df_processado.to_csv(arquivo_processado, index=False)\n",
    "dataset_processado.add_files(str(arquivo_processado))\n",
    "dataset_processado.upload()\n",
    "dataset_processado.finalize()\n",
    "\n",
    "print(f\"Dataset processado criado: ID = {dataset_processado.id}\")\n",
    "\n",
    "# Registrar como artefato na task\n",
    "task_processamento.upload_artifact(\n",
    "    \"dados_processados_sample\",\n",
    "    artifact_object=df_processado.head(100)  # Apenas amostra\n",
    ")\n",
    "\n",
    "task_processamento.close()\n",
    "print(\"Task de processamento finalizada!\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 3: PIPELINE DE TREINAMENTO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 3] Pipeline de Treinamento\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar task de treinamento (COM FIX PARA NOTEBOOKS)\n",
    "task_treinamento = Task.init(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=\"Pipeline_Treinamento\",\n",
    "    task_type=Task.TaskTypes.training,\n",
    "    reuse_last_task_id=False,\n",
    "    continue_last_task=False,\n",
    "    auto_connect_frameworks=False,\n",
    "    auto_resource_monitoring=False\n",
    ")\n",
    "\n",
    "# Conectar dataset processado\n",
    "task_treinamento.connect_configuration({\n",
    "    \"dataset_id\": dataset_processado.id,\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"tipo_problema\": \"regressao\"\n",
    "})\n",
    "\n",
    "# Preparar dados\n",
    "df_treino = df_processado.dropna()\n",
    "print(f\"Dados para treino: {df_treino.shape}\")\n",
    "\n",
    "# Executar treinamento\n",
    "print(\"Executando treinamento...\")\n",
    "resultado = treinar_pipeline_completo(\n",
    "    dados=df_treino,\n",
    "    coluna_alvo=coluna_alvo,\n",
    "    tipo_problema='regressao',\n",
    "    n_modelos_comparar=3,\n",
    "    otimizar_hiperparametros=True,\n",
    "    n_iter_otimizacao=10,\n",
    "    salvar_modelo_final=True,\n",
    "    nome_modelo=\"modelo_conforto_termico_clearml\",\n",
    "    pasta_modelos=\"../modelos\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 4: REGISTRO DE M√âTRICAS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 4] Registro de M√©tricas\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "logger = task_treinamento.get_logger()\n",
    "\n",
    "# M√©tricas do melhor modelo\n",
    "metricas = resultado['metricas_melhor']\n",
    "print(\"\\nMetricas registradas:\")\n",
    "for nome, valor in metricas.items():\n",
    "    if isinstance(valor, (int, float)):\n",
    "        logger.report_single_value(nome, valor)\n",
    "        print(f\"  {nome}: {valor:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 5: UPLOAD DE TABELAS E ARTEFATOS\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 5] Upload de Tabelas e Artefatos\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tabela de compara√ß√£o de modelos\n",
    "tabela_comparacao = resultado['tabela_comparacao']\n",
    "task_treinamento.upload_artifact(\n",
    "    \"comparacao_modelos\",\n",
    "    artifact_object=tabela_comparacao\n",
    ")\n",
    "print(\"Tabela de comparacao enviada como artefato\")\n",
    "\n",
    "# Criar gr√°fico de compara√ß√£o de modelos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "tabela_comparacao[['MAE', 'MSE', 'RMSE', 'R2']].plot(kind='bar', ax=ax)\n",
    "ax.set_title('Comparacao de Modelos - Metricas')\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Valor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Enviar gr√°fico\n",
    "logger.report_matplotlib_figure(\n",
    "    title=\"Comparacao de Modelos\",\n",
    "    series=\"Metricas\",\n",
    "    figure=fig,\n",
    "    iteration=0\n",
    ")\n",
    "plt.close()\n",
    "print(\"Grafico de comparacao enviado\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 6: REGISTRO DO MODELO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 6] Registro do Modelo\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Criar OutputModel\n",
    "melhor_modelo_nome = str(resultado['tabela_comparacao'].index[0])\n",
    "output_model = OutputModel(\n",
    "    task=task_treinamento,\n",
    "    name=f\"modelo_{melhor_modelo_nome}_conforto_termico\",\n",
    "    framework=\"PyCaret\"\n",
    ")\n",
    "\n",
    "# Registrar informa√ß√µes do modelo\n",
    "output_model.update_labels({\n",
    "    \"tipo\": \"regressao\",\n",
    "    \"coluna_alvo\": coluna_alvo,\n",
    "    \"melhor_modelo\": melhor_modelo_nome\n",
    "})\n",
    "\n",
    "# Adicionar m√©tricas ao modelo\n",
    "output_model.update_design(config_dict={\n",
    "    \"metricas\": {k: float(v) for k, v in metricas.items() if isinstance(v, (int, float))},\n",
    "    \"n_features\": df_treino.shape[1] - 1,\n",
    "    \"n_samples_treino\": df_treino.shape[0]\n",
    "})\n",
    "\n",
    "# Upload do arquivo do modelo\n",
    "if 'caminho_modelo' in resultado and resultado['caminho_modelo']:\n",
    "    caminho_modelo = Path(resultado['caminho_modelo'])\n",
    "    if caminho_modelo.exists():\n",
    "        output_model.update_weights(weights_filename=str(caminho_modelo))\n",
    "        print(f\"Modelo registrado: {caminho_modelo}\")\n",
    "    else:\n",
    "        print(f\"AVISO: Arquivo do modelo nao encontrado: {caminho_modelo}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ETAPA 7: FINALIZA√á√ÉO E RESUMO\n",
    "# ============================================================================\n",
    "print(\"\\n[ETAPA 7] Finalizacao\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Tags na task\n",
    "task_treinamento.add_tags([\n",
    "    \"producao\",\n",
    "    melhor_modelo_nome,\n",
    "    f\"r2_{metricas.get('R2', 0):.2f}\".replace(\".\", \"_\")\n",
    "])\n",
    "\n",
    "task_treinamento.close()\n",
    "\n",
    "# Limpar arquivos tempor√°rios\n",
    "import shutil\n",
    "if temp_path.exists():\n",
    "    shutil.rmtree(temp_path)\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMO FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETO FINALIZADO - TUDO REGISTRADO NO CLEARML!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProjeto: {PROJECT_NAME}\")\n",
    "print(f\"\\nDatasets criados:\")\n",
    "print(f\"  1. Dados brutos: {dataset_bruto.id}\")\n",
    "print(f\"  2. Dados processados: {dataset_processado.id}\")\n",
    "print(f\"\\nTasks criadas:\")\n",
    "print(f\"  1. Processamento: {task_processamento.id}\")\n",
    "print(f\"  2. Treinamento: {task_treinamento.id}\")\n",
    "print(f\"\\nModelo registrado:\")\n",
    "print(f\"  - Nome: modelo_{melhor_modelo_nome}_conforto_termico\")\n",
    "print(f\"  - Melhor modelo: {melhor_modelo_nome}\")\n",
    "print(f\"  - R2: {metricas.get('R2', 0):.4f}\")\n",
    "print(f\"  - MAE: {metricas.get('MAE', 0):.4f}\")\n",
    "print(\"\\nAcesse o ClearML UI para visualizar todos os resultados!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaf991",
   "metadata": {},
   "source": [
    "## Visualizar Resultados no ClearML\n",
    "\n",
    "Ap√≥s executar a c√©lula acima, acesse a interface web do ClearML:\n",
    "- **Local**: http://localhost:8080\n",
    "- **Demo**: https://app.clear.ml\n",
    "\n",
    "### O que voc√™ vai encontrar:\n",
    "\n",
    "**üìä No Projeto \"conforto_termico\":**\n",
    "- 2 Tasks (Processamento + Treinamento)\n",
    "- 2 Datasets (Bruto + Processado) com genealogia\n",
    "- 1 Modelo registrado com m√©tricas\n",
    "\n",
    "**üìà M√©tricas rastreadas:**\n",
    "- Todas as m√©tricas de regress√£o (MAE, MSE, RMSE, R2, etc.)\n",
    "- Compara√ß√£o entre modelos\n",
    "- Gr√°ficos de compara√ß√£o\n",
    "\n",
    "**üì¶ Artefatos salvos:**\n",
    "- Tabela de compara√ß√£o de modelos\n",
    "- Sample dos dados processados\n",
    "- Gr√°fico de m√©tricas\n",
    "\n",
    "**ü§ñ Modelo:**\n",
    "- Arquivo .pkl do modelo treinado\n",
    "- Labels e configura√ß√µes\n",
    "- Genealogia completa (dados ‚Üí processamento ‚Üí treinamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba750975",
   "metadata": {},
   "source": [
    "## [OPCIONAL] Buscar e Reusar Recursos do ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# REUSAR RECURSOS DO CLEARML (Datasets, Modelos, Tasks)\n",
    "# =========================================================================\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clearml import Task, Dataset, Model\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_NAME = \"conforto_termico_def\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUSCANDO RECURSOS NO CLEARML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LISTAR DATASETS DISPON√çVEIS\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Datasets disponiveis no projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "datasets = Dataset.list_datasets(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    partial_name=\"conforto_termico\"\n",
    ")\n",
    "\n",
    "for i, ds in enumerate(datasets, 1):\n",
    "    print(f\"\\n{i}. {ds.name}\")\n",
    "    print(f\"   ID: {ds.id}\")\n",
    "    print(f\"   Versao: {ds.version}\")\n",
    "    print(f\"   Criado: {ds.created}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. BAIXAR DATASET ESPEC√çFICO\n",
    "# ============================================================================\n",
    "print(\"\\n[2] Baixando dataset processado:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Buscar o dataset mais recente de dados processados\n",
    "dataset_processado = Dataset.get(\n",
    "    dataset_project=PROJECT_NAME,\n",
    "    dataset_name=\"dados_processados_conforto_termico\"\n",
    ")\n",
    "\n",
    "# Baixar para pasta local\n",
    "local_path = dataset_processado.get_local_copy()\n",
    "print(f\"Dataset baixado para: {local_path}\")\n",
    "\n",
    "# Carregar dados\n",
    "arquivos = list(Path(local_path).glob(\"*.csv\"))\n",
    "if arquivos:\n",
    "    df_downloaded = pd.read_csv(arquivos[0])\n",
    "    print(f\"Dados carregados: {df_downloaded.shape}\")\n",
    "    print(df_downloaded.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LISTAR TASKS DO PROJETO\n",
    "# ============================================================================\n",
    "print(\"\\n[3] Tasks do projeto:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "tasks = Task.get_tasks(\n",
    "    project_name=PROJECT_NAME,\n",
    "    task_name=None  # Todas as tasks\n",
    ")\n",
    "\n",
    "for i, t in enumerate(tasks[:5], 1):  # Primeiras 5\n",
    "    print(f\"\\n{i}. {t.name}\")\n",
    "    print(f\"   ID: {t.id}\")\n",
    "    print(f\"   Status: {t.status}\")\n",
    "    print(f\"   Tipo: {t.task_type}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUSCAR MODELOS REGISTRADOS\n",
    "# ============================================================================\n",
    "print(\"\\n[4] Modelos registrados:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "modelos = Model.query_models(\n",
    "    project_name=PROJECT_NAME,\n",
    "    model_name=None  # Todos os modelos\n",
    ")\n",
    "\n",
    "for i, modelo in enumerate(modelos, 1):\n",
    "    print(f\"\\n{i}. {modelo.name}\")\n",
    "    print(f\"   ID: {modelo.id}\")\n",
    "    print(f\"   Framework: {modelo.framework}\")\n",
    "    print(f\"   Labels: {modelo.labels}\")\n",
    "    \n",
    "    # Baixar modelo se quiser\n",
    "    # modelo_local = modelo.get_local_copy()\n",
    "    # print(f\"   Download: {modelo_local}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CLONAR E REUSAR TASK\n",
    "# ============================================================================\n",
    "print(\"\\n[5] Exemplo: Clonar uma task existente\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if tasks:\n",
    "    task_original = tasks[0]\n",
    "    print(f\"Clonando task: {task_original.name}\")\n",
    "    \n",
    "    # Clonar task (cria uma c√≥pia)\n",
    "    task_clonada = Task.clone(\n",
    "        source_task=task_original.id,\n",
    "        name=f\"{task_original.name}_clonada\",\n",
    "        project=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Task clonada criada: {task_clonada.id}\")\n",
    "    print(\"Voc√™ pode executar esta task clonada com configuracoes diferentes!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSCA DE RECURSOS CONCLUIDA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cffb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGN√ìSTICO COMPLETO\n",
      "================================================================================\n",
      "\n",
      "[1] Verificando arquivo .env...\n",
      "‚úì Arquivo .env encontrado: C:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\.env\n",
      "  Linhas no arquivo: 5\n",
      "  ‚Ä¢ CLEARML_WEB_HOST: ‚úì\n",
      "  ‚Ä¢ CLEARML_API_HOST: ‚úì\n",
      "  ‚Ä¢ CLEARML_FILES_HOST: ‚úì\n",
      "  ‚Ä¢ CLEARML_API_ACCESS_KEY: ***\n",
      "  ‚Ä¢ CLEARML_API_SECRET_KEY: ***\n",
      "\n",
      "[2] Testando carregamento de credenciais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:39:31,707 - INFO - M√≥dulo ClearML carregado com sucesso\n",
      "2026-02-09 22:39:31,714 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:39:31,715 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:39:31,715 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:39:31,716 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-09 22:39:31,714 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:39:31,715 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:39:31,715 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:39:31,716 - INFO - ‚úì ClearML configurado para uso ONLINE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Credenciais carregadas com sucesso\n",
      "\n",
      "[3] Testando importa√ß√£o do pipeline...\n",
      "‚úì Pipeline importado com sucesso\n",
      "\n",
      "[4] Verificando arquivo de dados...\n",
      "‚úì Arquivo encontrado: C:\\Users\\Readone\\Desktop\\tcc_mba_esalq_mlops\\dados\\2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "  Tamanho: 0.29 MB\n",
      "\n",
      "================================================================================\n",
      "Se tudo estiver ‚úì, execute a pr√≥xima c√©lula para rodar o pipeline\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# PASSO 1: Verificar Importa√ß√µes e Credenciais\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar path do projeto\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGN√ìSTICO COMPLETO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Verificar arquivo .env\n",
    "print(\"\\n[1] Verificando arquivo .env...\")\n",
    "env_path = Path('../.env')\n",
    "if env_path.exists():\n",
    "    print(f\"‚úì Arquivo .env encontrado: {env_path.resolve()}\")\n",
    "    # Ler conte√∫do (sem mostrar secrets)\n",
    "    with open(env_path, 'r') as f:\n",
    "        linhas = f.readlines()\n",
    "    print(f\"  Linhas no arquivo: {len(linhas)}\")\n",
    "    for linha in linhas:\n",
    "        if '=' in linha and not linha.startswith('#'):\n",
    "            chave = linha.split('=')[0].strip()\n",
    "            print(f\"  ‚Ä¢ {chave}: {'***' if 'KEY' in chave else '‚úì'}\")\n",
    "else:\n",
    "    print(f\"‚úó Arquivo .env N√ÉO encontrado em: {env_path.resolve()}\")\n",
    "\n",
    "# 2. Testar carregamento de credenciais\n",
    "print(\"\\n[2] Testando carregamento de credenciais...\")\n",
    "try:\n",
    "    from src.clearml.utils.credenciais_clearml import configurar_clearml_online\n",
    "    sucesso = configurar_clearml_online()\n",
    "    if sucesso:\n",
    "        print(\"‚úì Credenciais carregadas com sucesso\")\n",
    "    else:\n",
    "        print(\"‚úó Falha ao carregar credenciais\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao importar/configurar: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 3. Verificar importa√ß√£o do pipeline\n",
    "print(\"\\n[3] Testando importa√ß√£o do pipeline...\")\n",
    "try:\n",
    "    from src.clearml.pipelines_clearml import executar_pipeline_processamento_clearml\n",
    "    print(\"‚úì Pipeline importado com sucesso\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao importar pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. Verificar arquivo de dados\n",
    "print(\"\\n[4] Verificando arquivo de dados...\")\n",
    "arquivo_dados = Path('../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv')\n",
    "if arquivo_dados.exists():\n",
    "    print(f\"‚úì Arquivo encontrado: {arquivo_dados.resolve()}\")\n",
    "    print(f\"  Tamanho: {arquivo_dados.stat().st_size / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚úó Arquivo N√ÉO encontrado: {arquivo_dados.resolve()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Se tudo estiver ‚úì, execute a pr√≥xima c√©lula para rodar o pipeline\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2b9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:39:31,753 - INFO - Modo ONLINE: Iniciando integra√ß√£o ClearML\n",
      "2026-02-09 22:39:31,755 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:39:31,756 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:39:31,757 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:39:31,758 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-09 22:39:31,755 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:39:31,756 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:39:31,757 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:39:31,758 - INFO - ‚úì ClearML configurado para uso ONLINE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE DE PROCESSAMENTO - VERS√ÉO FINAL\n",
      "================================================================================\n",
      "Arquivo: ../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "Modo: ONLINE (ClearML)\n",
      "\n",
      "Melhorias:\n",
      "  ‚úì Carregamento robusto com load_dataframe() de src/utils/io\n",
      "  ‚úì Detec√ß√£o autom√°tica de delimitador\n",
      "  ‚úì M√©tricas de entrada/sa√≠da registradas\n",
      "================================================================================\n",
      "\n",
      "ClearML Task: created new task id=a4ae6d82b61443279bff4087ad551ed1\n",
      "ClearML Task: created new task id=a4ae6d82b61443279bff4087ad551ed1\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/a4ae6d82b61443279bff4087ad551ed1/output/log\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/a4ae6d82b61443279bff4087ad551ed1/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:39:42,955 - INFO - Task criada: Pipeline_Processamento (ID: a4ae6d82b61443279bff4087ad551ed1, Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS)\n",
      "2026-02-09 22:39:46,740 - INFO - ‚úì Task ClearML criada (ID: a4ae6d82b61443279bff4087ad551ed1)\n",
      "2026-02-09 22:39:46,743 - INFO - ================================================================================\n",
      "2026-02-09 22:39:46,745 - INFO - PIPELINE DE PROCESSAMENTO\n",
      "2026-02-09 22:39:46,745 - INFO - ================================================================================\n",
      "2026-02-09 22:39:46,746 - INFO - \n",
      "[1] Carregando dados do CSV...\n",
      "2026-02-09 22:39:46,746 - INFO -     Arquivo: ../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "2026-02-09 22:39:46,740 - INFO - ‚úì Task ClearML criada (ID: a4ae6d82b61443279bff4087ad551ed1)\n",
      "2026-02-09 22:39:46,743 - INFO - ================================================================================\n",
      "2026-02-09 22:39:46,745 - INFO - PIPELINE DE PROCESSAMENTO\n",
      "2026-02-09 22:39:46,745 - INFO - ================================================================================\n",
      "2026-02-09 22:39:46,746 - INFO - \n",
      "[1] Carregando dados do CSV...\n",
      "2026-02-09 22:39:46,746 - INFO -     Arquivo: ../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv\n",
      "2026-02-09 22:39:46,781 - INFO -     ‚úì Dados carregados: (1720, 40)\n",
      "2026-02-09 22:39:46,781 - INFO -     Colunas: 40\n",
      "2026-02-09 22:39:46,782 - INFO - \n",
      "[2] Executando pipeline de processamento local...\n",
      "2026-02-09 22:39:46,781 - INFO -     ‚úì Dados carregados: (1720, 40)\n",
      "2026-02-09 22:39:46,781 - INFO -     Colunas: 40\n",
      "2026-02-09 22:39:46,782 - INFO - \n",
      "[2] Executando pipeline de processamento local...\n",
      "2026-02-09 22:39:46,889 - INFO - ‚úì Processamento conclu√≠do\n",
      "2026-02-09 22:39:46,889 - INFO -   Shape final: (1720, 41)\n",
      "2026-02-09 22:39:46,890 - INFO - \n",
      "[3] Registrando artefatos no ClearML...\n",
      "2026-02-09 22:39:46,895 - INFO - M√©tricas registradas: ['linhas_entrada', 'colunas_entrada', 'linhas_saida', 'colunas_saida', 'nas_removidos']\n",
      "2026-02-09 22:39:46,889 - INFO - ‚úì Processamento conclu√≠do\n",
      "2026-02-09 22:39:46,889 - INFO -   Shape final: (1720, 41)\n",
      "2026-02-09 22:39:46,890 - INFO - \n",
      "[3] Registrando artefatos no ClearML...\n",
      "2026-02-09 22:39:46,895 - INFO - M√©tricas registradas: ['linhas_entrada', 'colunas_entrada', 'linhas_saida', 'colunas_saida', 'nas_removidos']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Iniciando pipeline de processamento BASE...\n",
      "  1Ô∏è‚É£ Aplicando substitui√ß√µes de limpeza...\n",
      "  2Ô∏è‚É£ Convertendo tipos de dados...\n",
      "  3Ô∏è‚É£ Imputando valores faltantes...\n",
      "  4Ô∏è‚É£ Criando agrupamento temporal...\n",
      "‚úÖ Pipeline BASE conclu√≠do! Shape final: (1720, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:39:49,995 - INFO - DataFrame registrado: dados_processados_sample (shape: (100, 41))\n",
      "2026-02-09 22:39:49,996 - INFO - ‚úì Artefatos registrados\n",
      "2026-02-09 22:39:49,997 - INFO - \n",
      "[4] Criando dataset ClearML...\n",
      "2026-02-09 22:39:49,996 - INFO - ‚úì Artefatos registrados\n",
      "2026-02-09 22:39:49,997 - INFO - \n",
      "[4] Criando dataset ClearML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/801c2956f7704234b987beef3f9cc1e6/experiments/eed3f4f2f711470882340222902528e9/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/801c2956f7704234b987beef3f9cc1e6/experiments/eed3f4f2f711470882340222902528e9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:40:14,877 - INFO - Dataset criado: dados_processados (ID: eed3f4f2f711470882340222902528e9, Projeto: Datasets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading dataset changes (1 files compressed to 49.62 KiB) to https://files.clear.ml\n",
      "File compression and upload completed: total size 49.62 KiB, 1 chunk(s) stored (average size 49.62 KiB)\n",
      "File compression and upload completed: total size 49.62 KiB, 1 chunk(s) stored (average size 49.62 KiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:40:42,137 - INFO - ‚úì Dataset criado (ID: eed3f4f2f711470882340222902528e9)\n",
      "2026-02-09 22:40:42,139 - INFO - \n",
      "[5] Fechando task...\n",
      "2026-02-09 22:40:42,139 - INFO - \n",
      "[5] Fechando task...\n",
      "2026-02-09 22:40:45,295 - INFO - ‚úì Task finalizada e pronta para pr√≥ximo pipeline\n",
      "2026-02-09 22:40:45,295 - INFO - \n",
      "================================================================================\n",
      "2026-02-09 22:40:45,296 - INFO - PIPELINE CONCLU√çDO COM SUCESSO\n",
      "2026-02-09 22:40:45,297 - INFO - ================================================================================\n",
      "2026-02-09 22:40:45,295 - INFO - ‚úì Task finalizada e pronta para pr√≥ximo pipeline\n",
      "2026-02-09 22:40:45,295 - INFO - \n",
      "================================================================================\n",
      "2026-02-09 22:40:45,296 - INFO - PIPELINE CONCLU√çDO COM SUCESSO\n",
      "2026-02-09 22:40:45,297 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PIPELINE CONCLU√çDO COM SUCESSO!\n",
      "================================================================================\n",
      "\n",
      "üìä Resultados:\n",
      "  Shape final: (1720, 41)\n",
      "  Dataset ID: eed3f4f2f711470882340222902528e9\n",
      "  NAs removidos: 435\n",
      "\n",
      "üåê Acesse: https://app.clear.ml\n",
      "  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\n",
      "  ‚Ä¢ Task: Pipeline_Processamento\n",
      "  ‚Ä¢ Dataset: dados_processados (ID: eed3f4f2...)\n",
      "  ‚Ä¢ M√©tricas: linhas entrada=1720, NAs removidos=435\n",
      "\n",
      "üíæ DataFrame processado dispon√≠vel em:\n",
      "  resultado['df_processado']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Processamento\n",
    "# ============================================================================\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_processamento_clearml\n",
    "\n",
    "arquivo = '../dados/2025.05.14_thermal_confort_santa_maria_brazil_.csv'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE DE PROCESSAMENTO - VERS√ÉO FINAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Arquivo: {arquivo}\")\n",
    "print(\"Modo: ONLINE (ClearML)\")\n",
    "print(\"\\nMelhorias:\")\n",
    "print(\"  ‚úì Carregamento robusto com load_dataframe() de src/utils/io\")\n",
    "print(\"  ‚úì Detec√ß√£o autom√°tica de delimitador\")\n",
    "print(\"  ‚úì M√©tricas de entrada/sa√≠da registradas\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    resultado = executar_pipeline_processamento_clearml(\n",
    "        caminho_csv=arquivo,\n",
    "        offline_mode=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ PIPELINE CONCLU√çDO COM SUCESSO!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"  Shape final: {resultado['shape']}\")\n",
    "    print(f\"  Dataset ID: {resultado.get('dataset_id', 'N/A')}\")\n",
    "    print(f\"  NAs removidos: {resultado.get('nas_removidos', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nüåê Acesse: https://app.clear.ml\")\n",
    "    print(f\"  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\")\n",
    "    print(f\"  ‚Ä¢ Task: Pipeline_Processamento\")\n",
    "    print(f\"  ‚Ä¢ Dataset: dados_processados (ID: {resultado.get('dataset_id', 'N/A')[:8]}...)\")\n",
    "    print(f\"  ‚Ä¢ M√©tricas: linhas entrada={resultado['shape'][0]}, NAs removidos={resultado.get('nas_removidos', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nüíæ DataFrame processado dispon√≠vel em:\")\n",
    "    print(\"  resultado['df_processado']\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå ERRO DETECTADO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTipo do erro: {type(e).__name__}\")\n",
    "    print(f\"Mensagem: {str(e)}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRACEBACK COMPLETO:\")\n",
    "    print(\"-\"*80)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e604989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:40:45,321 - INFO - Modo ONLINE: Iniciando integra√ß√£o ClearML\n",
      "2026-02-09 22:40:45,323 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:40:45,323 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:40:45,324 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:40:45,325 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-09 22:40:45,323 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:40:45,323 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:40:45,324 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:40:45,325 - INFO - ‚úì ClearML configurado para uso ONLINE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE DE FEATURES - CLEARML ONLINE\n",
      "================================================================================\n",
      "‚úì Pipeline 1 conclu√≠do\n",
      "  Dataset ID anterior: eed3f4f2f711470882340222902528e9\n",
      "\n",
      "Dados para features: (1720, 41)\n",
      "================================================================================\n",
      "\n",
      "ClearML Task: created new task id=3591669706a0414a90dd7cd5538de23a\n",
      "ClearML Task: created new task id=3591669706a0414a90dd7cd5538de23a\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/3591669706a0414a90dd7cd5538de23a/output/log\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/3591669706a0414a90dd7cd5538de23a/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:40:54,872 - INFO - Task criada: Pipeline_Features (ID: 3591669706a0414a90dd7cd5538de23a, Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS)\n",
      "2026-02-09 22:40:58,522 - INFO - ‚úì Task ClearML criada (ID: 3591669706a0414a90dd7cd5538de23a)\n",
      "2026-02-09 22:40:58,523 - INFO - ================================================================================\n",
      "2026-02-09 22:40:58,524 - INFO - PIPELINE DE ENGENHARIA DE FEATURES\n",
      "2026-02-09 22:40:58,526 - INFO - ================================================================================\n",
      "2026-02-09 22:40:58,527 - INFO - \n",
      "[1] Executando pipeline de features local...\n",
      "2026-02-09 22:40:58,527 - INFO -     Shape entrada: (1720, 41)\n",
      "2026-02-09 22:40:58,522 - INFO - ‚úì Task ClearML criada (ID: 3591669706a0414a90dd7cd5538de23a)\n",
      "2026-02-09 22:40:58,523 - INFO - ================================================================================\n",
      "2026-02-09 22:40:58,524 - INFO - PIPELINE DE ENGENHARIA DE FEATURES\n",
      "2026-02-09 22:40:58,526 - INFO - ================================================================================\n",
      "2026-02-09 22:40:58,527 - INFO - \n",
      "[1] Executando pipeline de features local...\n",
      "2026-02-09 22:40:58,527 - INFO -     Shape entrada: (1720, 41)\n",
      "2026-02-09 22:40:58,634 - INFO - ‚úì Features criadas com sucesso\n",
      "2026-02-09 22:40:58,635 - INFO -   Shape final: (1720, 78)\n",
      "2026-02-09 22:40:58,636 - INFO -   Novas colunas: 37\n",
      "2026-02-09 22:40:58,636 - INFO - \n",
      "[2] Registrando artefatos no ClearML...\n",
      "2026-02-09 22:40:58,634 - INFO - ‚úì Features criadas com sucesso\n",
      "2026-02-09 22:40:58,635 - INFO -   Shape final: (1720, 78)\n",
      "2026-02-09 22:40:58,636 - INFO -   Novas colunas: 37\n",
      "2026-02-09 22:40:58,636 - INFO - \n",
      "[2] Registrando artefatos no ClearML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Iniciando pipeline de FEATURES...\n",
      "  1Ô∏è‚É£ Criando features derivadas (5 tipos)...\n",
      "  2Ô∏è‚É£ Aplicando codifica√ß√£o (label)...\n",
      "  3Ô∏è‚É£ Aplicando normaliza√ß√£o (standard)...\n",
      "‚úÖ Pipeline FEATURES conclu√≠do! Shape final: (1720, 78)\n",
      "   Novas colunas criadas: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:41:01,883 - INFO - DataFrame registrado: dados_features (shape: (1720, 78))\n",
      "2026-02-09 22:41:03,446 - INFO - Arquivo registrado: artefatos_features\n",
      "2026-02-09 22:41:03,448 - INFO -   Artefatos registrados: ['mapeamentos_codificacao', 'colunas_normalizadas', 'artefatos_codificacao', 'artefatos_normalizacao']\n",
      "2026-02-09 22:41:03,449 - INFO - M√©tricas registradas: ['linhas_features', 'colunas_features', 'novas_colunas', 'colunas_entrada']\n",
      "2026-02-09 22:41:03,450 - INFO - ‚úì Artefatos registrados\n",
      "2026-02-09 22:41:03,451 - INFO - \n",
      "[3] Criando dataset ClearML...\n",
      "2026-02-09 22:41:03,446 - INFO - Arquivo registrado: artefatos_features\n",
      "2026-02-09 22:41:03,448 - INFO -   Artefatos registrados: ['mapeamentos_codificacao', 'colunas_normalizadas', 'artefatos_codificacao', 'artefatos_normalizacao']\n",
      "2026-02-09 22:41:03,449 - INFO - M√©tricas registradas: ['linhas_features', 'colunas_features', 'novas_colunas', 'colunas_entrada']\n",
      "2026-02-09 22:41:03,450 - INFO - ‚úì Artefatos registrados\n",
      "2026-02-09 22:41:03,451 - INFO - \n",
      "[3] Criando dataset ClearML...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sdiskpart' object has no attribute 'maxfile'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping upload, could not find object file 'c:/Users/Readone/Desktop/tcc_mba_esalq_mlops/notebooks/temp_artefatos_features.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: https://app.clear.ml/projects/d002871436ec4f3a8e158a0997a0e923/experiments/80272a4490934b8ba191125924fc6c34/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/d002871436ec4f3a8e158a0997a0e923/experiments/80272a4490934b8ba191125924fc6c34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:41:53,903 - INFO - Dataset criado: dados_features (ID: 80272a4490934b8ba191125924fc6c34, Projeto: Datasets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading dataset changes (1 files compressed to 137.64 KiB) to https://files.clear.ml\n",
      "File compression and upload completed: total size 137.64 KiB, 1 chunk(s) stored (average size 137.64 KiB)\n",
      "File compression and upload completed: total size 137.64 KiB, 1 chunk(s) stored (average size 137.64 KiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:43:28,236 - INFO - ‚úì Dataset criado (ID: 80272a4490934b8ba191125924fc6c34)\n",
      "2026-02-09 22:43:28,237 - INFO - \n",
      "================================================================================\n",
      "2026-02-09 22:43:28,238 - INFO - PIPELINE DE FEATURES CONCLU√çDO COM SUCESSO\n",
      "2026-02-09 22:43:28,239 - INFO - ================================================================================\n",
      "2026-02-09 22:43:28,237 - INFO - \n",
      "================================================================================\n",
      "2026-02-09 22:43:28,238 - INFO - PIPELINE DE FEATURES CONCLU√çDO COM SUCESSO\n",
      "2026-02-09 22:43:28,239 - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PIPELINE DE FEATURES CONCLU√çDO!\n",
      "================================================================================\n",
      "\n",
      "üìä Resultados:\n",
      "  Shape final: (1720, 78)\n",
      "  Dataset ID: 80272a44...\n",
      "  Artefatos criados: 4\n",
      "\n",
      "üì¶ Artefatos:\n",
      "  ‚Ä¢ mapeamentos_codificacao\n",
      "  ‚Ä¢ colunas_normalizadas\n",
      "  ‚Ä¢ artefatos_codificacao\n",
      "  ‚Ä¢ artefatos_normalizacao\n",
      "\n",
      "üåê Acesse: https://app.clear.ml\n",
      "  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\n",
      "  ‚Ä¢ Task: Pipeline_Features\n",
      "  ‚Ä¢ Dataset: dados_features (ID: 80272a44...)\n",
      "  ‚Ä¢ Genealogia: dados_brutos ‚Üí processados ‚Üí features\n",
      "\n",
      "üíæ DataFrame com features dispon√≠vel em:\n",
      "  resultado_features['dados_features']\n",
      "\n",
      "üìà Transforma√ß√µes aplicadas:\n",
      "  ‚úì Codifica√ß√£o: 2 vari√°veis\n",
      "  ‚úì Normaliza√ß√£o: 30 colunas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE 2: ENGENHARIA DE FEATURES COM CLEARML\n",
    "# ============================================================================\n",
    "# Recebe o resultado do Pipeline 1 e aplica transforma√ß√µes de features\n",
    "# ============================================================================\n",
    "\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_features_clearml\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE DE FEATURES - CLEARML ONLINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar se temos o resultado do pipeline anterior\n",
    "if 'resultado' not in locals():\n",
    "    print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Execute primeiro o Pipeline 1 (Processamento)\")\n",
    "    print(\"Usando fallback: carregando dados do notebook...\")\n",
    "    \n",
    "    # Fallback: usar df_raw do notebook\n",
    "    from src.pipelines.pipeline_processamento import executar_pipeline_processamento\n",
    "    df_para_features = executar_pipeline_processamento(df_raw)\n",
    "    dataset_anterior_id = None\n",
    "else:\n",
    "    print(f\"‚úì Pipeline 1 conclu√≠do\")\n",
    "    print(f\"  Dataset ID anterior: {resultado.get('dataset_id', 'N/A')}\")\n",
    "    df_para_features = resultado['dados_processados']\n",
    "    dataset_anterior_id = resultado.get('dataset_id')\n",
    "\n",
    "print(f\"\\nDados para features: {df_para_features.shape}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    resultado_features = executar_pipeline_features_clearml(\n",
    "        df_processado=df_para_features,\n",
    "        dataset_processado_id=dataset_anterior_id,\n",
    "        offline_mode=False,\n",
    "        # Configura√ß√µes de features\n",
    "        aplicar_codificacao=True,\n",
    "        metodo_codificacao='label',\n",
    "        criar_features_derivadas=True,\n",
    "        tipos_features_derivadas=['imc', 'imc_classe', 'heat_index', 'dew_point', 't*u'],\n",
    "        aplicar_normalizacao=True,\n",
    "        metodo_normalizacao='standard'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ PIPELINE DE FEATURES CONCLU√çDO!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"  Shape final: {resultado_features['shape']}\")\n",
    "    \n",
    "    # Formata√ß√£o segura do Dataset ID\n",
    "    dataset_id = resultado_features.get('dataset_id', 'N/A')\n",
    "    if dataset_id and dataset_id != 'N/A':\n",
    "        dataset_id_display = f\"{dataset_id[:8]}...\"\n",
    "    else:\n",
    "        dataset_id_display = \"N/A (modo offline ou erro na cria√ß√£o)\"\n",
    "    print(f\"  Dataset ID: {dataset_id_display}\")\n",
    "    \n",
    "    print(f\"  Artefatos criados: {len(resultado_features.get('artefatos', {}))}\")\n",
    "    \n",
    "    # Mostrar artefatos criados\n",
    "    if resultado_features.get('artefatos'):\n",
    "        print(f\"\\nüì¶ Artefatos:\")\n",
    "        for nome, artefato in resultado_features['artefatos'].items():\n",
    "            if isinstance(artefato, dict) and 'shape' in artefato:\n",
    "                print(f\"  ‚Ä¢ {nome}: {artefato['shape']}\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ {nome}\")\n",
    "    \n",
    "    print(f\"\\nüåê Acesse: https://app.clear.ml\")\n",
    "    print(f\"  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\")\n",
    "    print(f\"  ‚Ä¢ Task: Pipeline_Features\")\n",
    "    print(f\"  ‚Ä¢ Dataset: dados_features (ID: {dataset_id_display})\")\n",
    "    print(f\"  ‚Ä¢ Genealogia: dados_brutos ‚Üí processados ‚Üí features\")\n",
    "    \n",
    "    print(\"\\nüíæ DataFrame com features dispon√≠vel em:\")\n",
    "    print(\"  resultado_features['dados_features']\")\n",
    "    \n",
    "    # Estat√≠sticas das features criadas\n",
    "    if 'artefatos' in resultado_features:\n",
    "        print(f\"\\nüìà Transforma√ß√µes aplicadas:\")\n",
    "        if 'mapeamentos_codificacao' in resultado_features['artefatos']:\n",
    "            print(f\"  ‚úì Codifica√ß√£o: {len(resultado_features['artefatos']['mapeamentos_codificacao'])} vari√°veis\")\n",
    "        if 'colunas_normalizadas' in resultado_features['artefatos']:\n",
    "            print(f\"  ‚úì Normaliza√ß√£o: {len(resultado_features['artefatos']['colunas_normalizadas'])} colunas\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå ERRO DETECTADO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTipo do erro: {type(e).__name__}\")\n",
    "    print(f\"Mensagem: {str(e)}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRACEBACK COMPLETO:\")\n",
    "    print(\"-\"*80)\n",
    "    traceback.print_exc()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üí° Dicas:\")\n",
    "    print(\"  ‚Ä¢ Execute primeiro o Pipeline 1 (Processamento)\")\n",
    "    print(\"  ‚Ä¢ Verifique se o DataFrame est√° no formato correto\")\n",
    "    print(\"  ‚Ä¢ Use offline_mode=True para executar sem ClearML\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cf234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE DE TREINAMENTO - CLEARML ONLINE\n",
      "================================================================================\n",
      "‚úì Pipeline 2 conclu√≠do\n",
      "  Dataset ID anterior: 80272a4490934b8ba191125924fc6c34\n",
      "PIPELINE DE TREINAMENTO - CLEARML ONLINE\n",
      "================================================================================\n",
      "‚úì Pipeline 2 conclu√≠do\n",
      "  Dataset ID anterior: 80272a4490934b8ba191125924fc6c34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:53:49,513 - INFO - Modo ONLINE: Iniciando integra√ß√£o ClearML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados para treinamento: (1720, 78)\n",
      "Coluna alvo: p1\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:53:49,738 - INFO - ‚úì Credenciais ClearML carregadas do .env\n",
      "2026-02-09 22:53:49,739 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:53:49,739 - INFO -   API Host: https://api.clear.ml\n",
      "2026-02-09 22:53:49,740 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:53:49,740 - INFO -   Web Host: https://app.clear.ml\n",
      "2026-02-09 22:53:49,818 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-09 22:53:49,818 - INFO - ‚úì ClearML configurado para uso ONLINE\n",
      "2026-02-09 22:53:49,879 - INFO - Fechando task anterior: Pipeline_Treinamento\n",
      "2026-02-09 22:53:49,879 - INFO - Fechando task anterior: Pipeline_Treinamento\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=563165a5492d44119a249e32223ada39\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/563165a5492d44119a249e32223ada39/output/log\n",
      "ClearML results page: https://app.clear.ml/projects/06661800c2e642339d895b6d02c06481/experiments/563165a5492d44119a249e32223ada39/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 22:54:09,465 - INFO - Task criada: Pipeline_Treinamento (ID: 563165a5492d44119a249e32223ada39, Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE 3: TREINAMENTO DE MODELOS COM CLEARML\n",
    "# ============================================================================\n",
    "# Recebe o resultado do Pipeline 2 e treina modelos com PyCaret\n",
    "# ============================================================================\n",
    "\n",
    "import traceback\n",
    "from src.clearml.pipelines_clearml import executar_pipeline_treinamento_clearml\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE DE TREINAMENTO - CLEARML ONLINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar se temos o resultado do pipeline anterior\n",
    "if 'resultado_features' not in locals():\n",
    "    print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Execute primeiro os Pipelines 1 e 2\")\n",
    "    print(\"Usando fallback: tentando usar dados do notebook...\")\n",
    "    \n",
    "    # Fallback: verificar se temos df_feat do notebook\n",
    "    if 'df_feat' in locals():\n",
    "        print(\"‚úì Usando df_feat do notebook\")\n",
    "        df_para_treino = df_feat\n",
    "        dataset_anterior_id = None\n",
    "    else:\n",
    "        print(\"‚ùå Nenhum dado dispon√≠vel. Execute os pipelines anteriores primeiro!\")\n",
    "        df_para_treino = None\n",
    "        dataset_anterior_id = None\n",
    "else:\n",
    "    print(f\"‚úì Pipeline 2 conclu√≠do\")\n",
    "    print(f\"  Dataset ID anterior: {resultado_features.get('dataset_id', 'N/A')}\")\n",
    "    df_para_treino = resultado_features['dados_features']\n",
    "    dataset_anterior_id = resultado_features.get('dataset_id')\n",
    "\n",
    "if df_para_treino is not None:\n",
    "    # Preparar dados (remover NAs)\n",
    "    df_treino_final = df_para_treino.dropna()\n",
    "    \n",
    "    print(f\"\\nDados para treinamento: {df_treino_final.shape}\")\n",
    "    print(f\"Coluna alvo: p1\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        resultado_treinamento = executar_pipeline_treinamento_clearml(\n",
    "            df_features=df_treino_final,\n",
    "            coluna_alvo='p1',\n",
    "            tipo_problema='regressao',\n",
    "            dataset_features_id=dataset_anterior_id,\n",
    "            offline_mode=False,\n",
    "            # Configura√ß√µes de treinamento\n",
    "            n_modelos_comparar=3,\n",
    "            otimizar_hiperparametros=True,\n",
    "            n_iter_otimizacao=10,\n",
    "            salvar_modelo_final=True,\n",
    "            nome_modelo=\"modelo_conforto_termico\",\n",
    "            pasta_modelos=\"../modelos\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ PIPELINE DE TREINAMENTO CONCLU√çDO!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Melhor modelo\n",
    "        melhor_modelo_nome = str(resultado_treinamento['tabela_comparacao'].index[0])\n",
    "        print(f\"\\nüèÜ Melhor Modelo: {melhor_modelo_nome}\")\n",
    "        \n",
    "        # M√©tricas\n",
    "        print(f\"\\nüìä M√©tricas do Melhor Modelo:\")\n",
    "        metricas = resultado_treinamento['metricas_melhor']\n",
    "        metricas_principais = ['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']\n",
    "        for nome in metricas_principais:\n",
    "            if nome in metricas:\n",
    "                valor = metricas[nome]\n",
    "                if isinstance(valor, (int, float)):\n",
    "                    print(f\"  ‚Ä¢ {nome}: {valor:.4f}\")\n",
    "        \n",
    "        # Model ID\n",
    "        model_id = resultado_treinamento.get('model_id', 'N/A')\n",
    "        if model_id and model_id != 'N/A':\n",
    "            model_id_display = model_id\n",
    "        else:\n",
    "            model_id_display = \"N/A (modo offline ou erro)\"\n",
    "        \n",
    "        print(f\"\\nüíæ Modelo:\")\n",
    "        print(f\"  ‚Ä¢ Salvo em: {resultado_treinamento.get('caminho_modelo', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ ClearML ID: {model_id_display}\")\n",
    "        \n",
    "        # Compara√ß√£o de modelos\n",
    "        print(f\"\\nüìã Compara√ß√£o de Modelos (Top {len(resultado_treinamento['tabela_comparacao'])}):\")\n",
    "        print(resultado_treinamento['tabela_comparacao'][['MAE', 'MSE', 'RMSE', 'R2']].head())\n",
    "        \n",
    "        print(f\"\\nüåê Acesse: https://app.clear.ml\")\n",
    "        print(f\"  ‚Ä¢ Projeto: Estudo de Sensa√ß√£o T√©rmica Humana em Santa Maria - RS\")\n",
    "        print(f\"  ‚Ä¢ Task: Pipeline_Treinamento\")\n",
    "        print(f\"  ‚Ä¢ Modelo: modelo_{melhor_modelo_nome}_modelo_conforto_termico\")\n",
    "        print(f\"  ‚Ä¢ Genealogia: dados_brutos ‚Üí processados ‚Üí features ‚Üí **MODELO**\")\n",
    "        \n",
    "        print(\"\\nüéØ Modelo treinado dispon√≠vel em:\")\n",
    "        print(\"  resultado_treinamento['melhor_modelo']\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ùå ERRO DETECTADO\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nTipo do erro: {type(e).__name__}\")\n",
    "        print(f\"Mensagem: {str(e)}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"TRACEBACK COMPLETO:\")\n",
    "        print(\"-\"*80)\n",
    "        traceback.print_exc()\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üí° Dicas:\")\n",
    "        print(\"  ‚Ä¢ Execute os Pipelines 1 e 2 primeiro\")\n",
    "        print(\"  ‚Ä¢ Verifique se o DataFrame tem a coluna alvo\")\n",
    "        print(\"  ‚Ä¢ Use offline_mode=True para executar sem ClearML\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå ERRO: Nenhum dado dispon√≠vel\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüí° Execute os pipelines na ordem:\")\n",
    "    print(\"  1. Pipeline 1: Processamento\")\n",
    "    print(\"  2. Pipeline 2: Features\")\n",
    "    print(\"  3. Pipeline 3: Treinamento (esta c√©lula)\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc-mba-esalq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
